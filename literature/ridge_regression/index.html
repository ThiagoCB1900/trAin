<!DOCTYPE html>
<html lang="pt-BR">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Ridge Regression (Regressão Ridge) - trAIn Documentation</title>
</head>
<body style="font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; line-height: 1.6; color: #333;">

<style>
  .card {
    border: 1px solid rgba(120, 120, 120, 0.35);
    border-radius: 8px;
    padding: 12px;
    margin: 10px 0;
  }
  .callout {
    border-left: 4px solid #1976d2;
    background: rgba(25, 118, 210, 0.08);
    border-radius: 6px;
    padding: 10px 12px;
    margin: 10px 0;
  }
  .warning {
    border-left: 4px solid #d32f2f;
    background: rgba(211, 47, 47, 0.08);
    border-radius: 6px;
    padding: 10px 12px;
    margin: 10px 0;
  }
  .formula-box {
    border: 1px solid rgba(120, 120, 120, 0.35);
    border-radius: 8px;
    padding: 10px 12px;
    margin: 10px 0;
  }
  .formula-title {
    font-weight: bold;
    margin-bottom: 6px;
  }
  .formula {
    font-family: "Cambria Math", "STIX Two Math", "Times New Roman", serif;
    font-size: 15px;
    background: rgba(120, 120, 120, 0.08);
    border-radius: 6px;
    padding: 6px 8px;
    margin: 6px 0;
  }
  table {
    width: 100%;
    border-collapse: collapse;
    margin: 10px 0;
  }
  table th, table td {
    border: 1px solid rgba(120, 120, 120, 0.35);
    padding: 8px;
    text-align: left;
  }
  table th {
    background: rgba(200, 200, 200, 0.1);
  }
  a {
    color: #1976d2;
    text-decoration: none;
  }
  a:hover {
    text-decoration: underline;
  }
</style>

<h2 style="margin-top:0;">Ridge Regression (Regressão Ridge)</h2>
<p>
  <b>Ridge Regression</b> é uma extensão da regressão linear que adiciona <b>regularização L2</b>
  para prevenir overfitting e estabilizar coeficientes quando há multicolinearidade. É
  especialmente útil quando features são correlacionadas ou o número de features é alto
  em relação ao número de observações. [1][2][3]
</p>

<div class="callout">
  <b>Em uma frase:</b> Ridge adiciona penalidade proporcional à soma dos quadrados dos
  coeficientes (L2), "encolhendo" coeficientes grandes e tornando o modelo mais estável
  e generalizável, sem eliminar features (ao contrário do Lasso).
</div>

<p>
  <b>Contexto histórico:</b> desenvolvida por Hoerl e Kennard em 1970 para resolver
  problemas de multicolinearidade em sistemas de equações mal-condicionadas. [1] O nome
  "Ridge" vem de adicionar uma "crista" (ridge) à diagonal da matriz XᵀX, tornando-a
  invertível mesmo quando features são correlacionadas. Ridge é também chamada de
  <b>Tikhonov regularization</b> (matemática aplicada) ou <b>weight decay</b> (redes neurais).
</p>

<h3 id="sumario">Sumário</h3>
<ul>
  <li><a href="#visao-geral">Visão geral</a></li>
  <li><a href="#porque-funciona">Por que funciona</a></li>
  <li><a href="#glossario">Glossário</a></li>
  <li><a href="#diferencial">Diferença vs Linear Regression</a></li>
  <li><a href="#matematica">Matemática por trás</a></li>
  <li><a href="#regularizacao">Regularização L2 detalhada</a></li>
  <li><a href="#bias-variance">Trade-off bias-variance</a></li>
  <li><a href="#multicolinearidade">Solução para multicolinearidade</a></li>
  <li><a href="#hiperparametros">Hiperparâmetros e impactos</a></li>
  <li><a href="#alpha">Escolha de alpha (crítico)</a></li>
  <li><a href="#solvers">Solvers: qual usar</a></li>
  <li><a href="#normalizacao">Normalização (OBRIGATÓRIA)</a></li>
  <li><a href="#interpretabilidade">Interpretabilidade</a></li>
  <li><a href="#avaliacao">Como avaliar o modelo</a></li>
  <li><a href="#saude">Aplicações em saúde</a></li>
  <li><a href="#exemplos">Exemplos publicados</a></li>
  <li><a href="#quando-usar">Quando usar (e quando evitar)</a></li>
  <li><a href="#boas-praticas">Boas práticas clínicas</a></li>
  <li><a href="#diagnostico">Diagnóstico de problemas</a></li>
  <li><a href="#perguntas">Perguntas frequentes</a></li>
  <li><a href="#comparacao">Comparação: Ridge vs Lasso vs Elastic Net</a></li>
  <li><a href="#leitura">Leitura recomendada</a></li>
  <li><a href="#referencias">Referências</a></li>
</ul>

<h3 id="visao-geral">Visão geral</h3>
<p>
  Ridge Regression modifica a regressão linear adicionando penalidade L2 aos coeficientes:
</p>

<div class="formula-box">
  <div class="formula-title">OLS (Linear Regression):</div>
  <div class="formula">minimize Σ (yᵢ - β₀ - Σ βⱼxᵢⱼ)²</div>
  
  <div class="formula-title" style="margin-top: 12px;">Ridge Regression:</div>
  <div class="formula">minimize Σ (yᵢ - β₀ - Σ βⱼxᵢⱼ)² + α Σ βⱼ²</div>
  <p style="font-size: 13px;">
    α = hiperparâmetro de regularização (controla força da penalidade)<br/>
    Σ βⱼ² = soma dos quadrados dos coeficientes (norma L2 ao quadrado)
  </p>
</div>

<p><b>Características principais:</b></p>
<ul>
  <li><b>Estabiliza coeficientes:</b> reduz variância quando features são correlacionadas.</li>
  <li><b>Nunca zera coeficientes:</b> todos coeficientes são encolhidos mas permanecem no modelo.</li>
  <li><b>Solução fechada:</b> possui fórmula analítica (não precisa otimização iterativa para α fixo).</li>
  <li><b>Interpretabilidade mantida:</b> coeficientes ainda têm significado direto (embora "encolhidos").</li>
  <li><b>Robustez:</b> menos sensível a outliers e multicolinearidade que OLS.</li>
  <li><b>Trade-off bias-variance:</b> adiciona bias para reduzir variance (melhora generalização).</li>
</ul>

<div class="callout">
  <b>Contexto clínico:</b> Ridge é amplamente usado em medicina quando há múltiplas features
  correlacionadas (ex.: marcadores inflamatórios, sinais vitais relacionados) ou quando
  número de features é próximo ao número de pacientes. Estudos mostram que Ridge frequentemente
  melhora predição em 5-15% sobre OLS em datasets clínicos típicos. [4][5][6]
</div>

<h3 id="porque-funciona">Para quem não conhece ML: por que isso funciona?</h3>
<p>
  Imagine que você está ajustando um modelo para predizer tempo de internação usando
  10 features (idade, comorbidades, labs, etc.). Com regressão linear normal (OLS), se
  duas features são muito correlacionadas (ex.: creatinina e TFG), o modelo pode ficar
  "confuso" e dar peso enorme para uma e peso negativo gigante para a outra, apenas para
  cancelar um ao outro. Isso causa instabilidade: pequenas mudanças nos dados causam
  mudanças enormes nos coeficientes.
</p>

<p>
  Ridge adiciona uma "multa" para coeficientes grandes. É como dizer ao modelo: "ok, você
  pode usar essas features correlacionadas, mas não pode dar peso MUITO grande para nenhuma
  delas". Isso força o modelo a distribuir o peso de forma mais equilibrada, criando
  coeficientes menores e mais estáveis.
</p>

<p>
  <b>Analogia:</b> imagine equilibrar um peso em uma prancha. Com OLS, o modelo pode colocar
  peso gigante de um lado e peso gigante negativo do outro (instável). Com Ridge, o modelo
  é forçado a distribuir peso de forma mais uniforme (estável).
</p>

<h3 id="glossario">Glossário rápido</h3>
<ul>
  <li><b>Regularização L2:</b> penalidade proporcional à soma dos quadrados dos coeficientes.</li>
  <li><b>Alpha (α):</b> força da regularização. α=0 = OLS, α→∞ = coeficientes→0.</li>
  <li><b>Lambda (λ):</b> mesmo que alpha (notação alternativa, mais comum em teoria).</li>
  <li><b>Shrinkage:</b> "encolhimento" dos coeficientes em direção a zero.</li>
  <li><b>Bias-variance tradeoff:</b> Ridge aumenta bias (erro sistemático) mas diminui variance (instabilidade).</li>
  <li><b>Multicolinearidade:</b> correlação entre features, causa instabilidade em OLS.</li>
  <li><b>VIF (Variance Inflation Factor):</b> métrica de multicolinearidade. Ridge reduz VIF efetivo.</li>
  <li><b>Condition number:</b> medida de quão mal-condicionada é a matriz XᵀX. Ridge reduz drasticamente.</li>
</ul>

<h3 id="diferencial">Diferença vs Linear Regression (OLS)</h3>

<table>
  <tr style="background: rgba(200, 200, 200, 0.1);">
    <th align="left"><b>Aspecto</b></th>
    <th align="left"><b>Linear Regression (OLS)</b></th>
    <th align="left"><b>Ridge Regression</b></th>
  </tr>
  <tr>
    <td><b>Função objetivo</b></td>
    <td>Minimiza apenas SSE</td>
    <td>Minimiza SSE + α·L2</td>
  </tr>
  <tr>
    <td><b>Multicolinearidade</b></td>
    <td>Coeficientes instáveis, erros padrão inflados</td>
    <td>Estabiliza coeficientes, reduz variância</td>
  </tr>
  <tr>
    <td><b>Número de features</b></td>
    <td>Requer p < n (não funciona se p ≥ n)</td>
    <td>Funciona mesmo se p > n (sempre invertível)</td>
  </tr>
  <tr>
    <td><b>Overfitting</b></td>
    <td>Pode overfittar se p/n alto</td>
    <td>Reduz overfitting via regularização</td>
  </tr>
  <tr>
    <td><b>Coeficientes</b></td>
    <td>Sem restrições (podem ser muito grandes)</td>
    <td>Encolhidos (menores em magnitude)</td>
  </tr>
  <tr>
    <td><b>Feature selection</b></td>
    <td>Não (manual via p-valores)</td>
    <td>Não (mantém todas features, apenas encolhe)</td>
  </tr>
  <tr>
    <td><b>Interpretabilidade</b></td>
    <td>Total</td>
    <td>Alta (mas coeficientes "encolhidos")</td>
  </tr>
  <tr>
    <td><b>Solução</b></td>
    <td>β = (XᵀX)⁻¹Xᵀy</td>
    <td>β = (XᵀX + αI)⁻¹Xᵀy</td>
  </tr>
  <tr>
    <td><b>Quando usar</b></td>
    <td>p pequeno (< 10), VIF < 5, baseline</td>
    <td>Multicolinearidade, p moderado-alto, p/n > 0.1</td>
  </tr>
</table>

<div class="callout">
  <b>Resumo prático:</b> use Ridge quando OLS falha devido a multicolinearidade (VIF > 10)
  ou quando p é grande em relação a n (p/n > 0.1). Ridge quase sempre generaliza melhor
  que OLS em dados reais, com custo mínimo de interpretabilidade.
</div>

<h3 id="matematica">Matemática por trás do algoritmo</h3>

<p>
  Ridge modifica a solução de OLS adicionando termo de regularização L2. A derivação é
  similar a OLS, mas com termo extra α·I na matriz. [1][2]
</p>

<div class="formula-box">
  <div class="formula-title">1. Função objetivo Ridge</div>
  <div class="formula">L(β) = ||y - Xβ||² + α||β||²</div>
  <div class="formula">L(β) = (y - Xβ)ᵀ(y - Xβ) + α·βᵀβ</div>
  <p style="font-size: 13px;">
    Primeiro termo: erro quadrático (mesmo que OLS)<br/>
    Segundo termo: penalidade L2 (novidade do Ridge)
  </p>
</div>

<div class="formula-box">
  <div class="formula-title">2. Solução analítica (forma fechada)</div>
  <div class="formula">β_ridge = (XᵀX + αI)⁻¹Xᵀy</div>
  <p style="font-size: 13px;">
    I = matriz identidade p×p<br/>
    α = hiperparâmetro de regularização<br/>
    <br/>
    Compare com OLS: β_OLS = (XᵀX)⁻¹Xᵀy<br/>
    A diferença é apenas o termo +αI na matriz
  </p>
</div>

<div class="formula-box">
  <div class="formula-title">3. Por que αI resolve multicolinearidade</div>
  <p style="font-size: 13px;">
    Quando features são correlacionadas, XᵀX tem autovalores próximos de zero (quase singular).<br/>
    Adicionar αI aumenta TODOS os autovalores por α, tornando matriz bem-condicionada.<br/>
    <br/>
    Condition number de XᵀX pode ser 10¹⁰ (praticamente não-invertível).<br/>
    Condition number de XᵀX + αI cai para 10²-10³ (estável).
  </p>
</div>

<div class="formula-box">
  <div class="formula-title">4. Relação com SVD (Singular Value Decomposition)</div>
  <div class="formula">X = UΣVᵀ (decomposição SVD)</div>
  <div class="formula">β_OLS = Σ (σᵢ⁻¹ uᵢᵀy) vᵢ</div>
  <div class="formula">β_ridge = Σ (σᵢ/(σᵢ² + α) uᵢᵀy) vᵢ</div>
  <p style="font-size: 13px;">
    σᵢ = valores singulares de X<br/>
    <br/>
    Ridge substitui 1/σᵢ (OLS) por σᵢ/(σᵢ² + α).<br/>
    Para σᵢ grandes: σᵢ/(σᵢ² + α) ≈ 1/σᵢ (quase sem mudança)<br/>
    Para σᵢ pequenos: σᵢ/(σᵢ² + α) ≈ σᵢ/α ≈ 0 (encolhe fortemente)<br/>
    <br/>
    <b>Interpretação:</b> Ridge "desliga" direções ruidosas (σᵢ pequenos) mantendo sinais fortes (σᵢ grandes).
  </p>
</div>

<h4>Derivação simplificada</h4>
<div class="card">
  <p style="font-size: 13px;">
    1. Função objetivo Ridge: L = (y - Xβ)ᵀ(y - Xβ) + α·βᵀβ<br/>
    <br/>
    2. Expandindo:<br/>
    &nbsp;&nbsp;&nbsp;L = yᵀy - 2yᵀXβ + βᵀXᵀXβ + α·βᵀβ<br/>
    <br/>
    3. Derivada em relação a β:<br/>
    &nbsp;&nbsp;&nbsp;∂L/∂β = -2Xᵀy + 2XᵀXβ + 2αβ<br/>
    <br/>
    4. Igualando a zero (mínimo):<br/>
    &nbsp;&nbsp;&nbsp;-2Xᵀy + 2XᵀXβ + 2αβ = 0<br/>
    &nbsp;&nbsp;&nbsp;XᵀXβ + αβ = Xᵀy<br/>
    &nbsp;&nbsp;&nbsp;(XᵀX + αI)β = Xᵀy<br/>
    <br/>
    5. Resolvendo para β:<br/>
    &nbsp;&nbsp;&nbsp;β = (XᵀX + αI)⁻¹Xᵀy
  </p>
</div>

<h3 id="regularizacao">Regularização L2 detalhada</h3>

<p>
  A penalidade L2 tem propriedades geométricas e estatísticas importantes que explicam
  por que Ridge funciona tão bem. [1][2][3]
</p>

<h4>Interpretação geométrica</h4>
<p>
  Ridge pode ser visto como <b>regressão com restrição</b>: encontre β que minimiza SSE
  sujeito a ||β||² ≤ t (para algum t relacionado a α).
</p>

<div class="card">
  <p style="font-size: 13px;">
    <b>Visualização 2D (2 coeficientes):</b><br/>
    • OLS: encontra ponto que minimiza SSE (centro das elipses de nível)<br/>
    • Ridge: encontra ponto onde elipse de SSE toca círculo ||β||² = t<br/>
    • Lasso: encontra ponto onde elipse toca losango |β| = t<br/>
    <br/>
    Ridge favorece círculo → coeficientes equilibrados, raramente zera<br/>
    Lasso favorece losango → coeficientes nos cantos (frequentemente zera)
  </p>
</div>

<h4>Por que L2 (quadrado) e não L1 (absoluto)?</h4>
<ul>
  <li><b>L2 é diferenciável:</b> tem derivada suave, fácil de otimizar.</li>
  <li><b>L2 tem solução fechada:</b> não precisa otimização iterativa.</li>
  <li><b>L2 distribui penalidade:</b> prefere 100 coefs pequenos vs 10 grandes + 90 zeros.</li>
  <li><b>L2 é rotacionalmente invariante:</b> não favorece eixos específicos.</li>
</ul>

<h4>Interpretação Bayesiana</h4>
<p>
  Ridge é equivalente a <b>Maximum A Posteriori (MAP)</b> com prior Gaussiano nos coeficientes:
</p>

<div class="formula-box">
  <div class="formula">β ~ Normal(0, σ²/α · I)</div>
  <p style="font-size: 13px;">
    Assumir prior Gaussiano centrado em zero é equivalente a penalidade L2.<br/>
    α controla quão "apertado" é o prior (quanto confiamos que coeficientes são pequenos).
  </p>
</div>

<h3 id="bias-variance">Trade-off bias-variance</h3>

<p>
  Ridge adiciona <b>bias</b> (erro sistemático) para reduzir <b>variance</b> (instabilidade).
  Isso frequentemente melhora erro total (MSE) apesar de aumentar bias. [2][7]
</p>

<div class="formula-box">
  <div class="formula">MSE = Bias² + Variance + Irreducible Error</div>
  <p style="font-size: 13px;">
    <b>OLS:</b> bias baixo, variance alta (se multicolinearidade ou p/n alto)<br/>
    <b>Ridge:</b> bias moderado, variance baixa<br/>
    <br/>
    Trade-off ideal: pequeno aumento em bias, grande redução em variance → MSE menor
  </p>
</div>

<table>
  <tr style="background: rgba(200, 200, 200, 0.1);">
    <th align="left"><b>Cenário</b></th>
    <th align="left"><b>OLS</b></th>
    <th align="left"><b>Ridge</b></th>
    <th align="left"><b>Resultado</b></th>
  </tr>
  <tr>
    <td>Features independentes, p/n baixo</td>
    <td>Bias baixo, Var baixa</td>
    <td>Bias moderado, Var baixa</td>
    <td>OLS ≈ Ridge (empate)</td>
  </tr>
  <tr>
    <td>Multicolinearidade moderada</td>
    <td>Bias baixo, Var alta</td>
    <td>Bias moderado, Var moderada</td>
    <td><b>Ridge vence</b> (5-10% melhor MSE)</td>
  </tr>
  <tr>
    <td>Multicolinearidade severa</td>
    <td>Bias baixo, Var muito alta</td>
    <td>Bias moderado, Var baixa</td>
    <td><b>Ridge vence</b> (10-30% melhor MSE)</td>
  </tr>
  <tr>
    <td>p/n > 0.5 (muitas features)</td>
    <td>Overfitting severo</td>
    <td>Bias moderado, Var moderada</td>
    <td><b>Ridge vence</b> (20-50% melhor MSE)</td>
  </tr>
</table>

<div class="callout">
  <b>Regra prática:</b> Ridge quase sempre melhora MSE em dados reais porque datasets
  clínicos tipicamente têm alguma multicolinearidade e p/n não muito baixo. O custo
  (pequeno aumento em bias) é compensado por grande redução em variance.
</div>

<h3 id="multicolinearidade">Solução para multicolinearidade</h3>

<p>
  Este é o <b>principal uso</b> de Ridge: estabilizar coeficientes quando features são
  correlacionadas. [1][4]
</p>

<h4>Como Ridge resolve multicolinearidade</h4>
<ol>
  <li><b>Estabiliza inversão de matriz:</b> XᵀX + αI é sempre invertível, mesmo se XᵀX é singular</li>
  <li><b>Reduz condition number:</b> de 10⁸-10¹⁰ (OLS) para 10²-10³ (Ridge com α apropriado)</li>
  <li><b>Distribui peso entre features correlacionadas:</b> ao invés de peso gigante em uma e negativo em outra, Ridge dá pesos moderados equilibrados</li>
  <li><b>Reduz VIF efetivo:</b> embora VIF calculado nas features originais permaneça alto, efeito prático (variância dos coeficientes) é reduzido</li>
</ol>

<h4>Exemplo numérico</h4>
<div class="card">
  <p style="font-size: 13px;"><b>Problema:</b> predizer tempo de internação com creatinina e TFG (altamente correlacionadas, r = -0.95)</p>
  <pre style="font-size: 12px; background: rgba(120,120,120,0.08); padding: 6px; border-radius: 4px;">
<b>OLS:</b>
Tempo = 5.2 + 8.3×Creatinina - 0.09×TFG
         (SE: 0.5)  (SE: 4.2)      (SE: 0.05)

Problema: coeficientes instáveis (SE muito altos), sinais contraintuitivos

<b>Ridge (α=1.0):</b>
Tempo = 5.1 + 2.1×Creatinina - 0.02×TFG
         (SE: 0.3)  (SE: 0.9)      (SE: 0.01)

Melhoria: coeficientes menores e estáveis (SE reduzidos 4-5x), sinais plausíveis
  </pre>
</div>

<h4>Quando multicolinearidade é problema</h4>
<ul>
  <li><b>VIF > 10:</b> Ridge obrigatório ou remova features correlacionadas</li>
  <li><b>VIF 5-10:</b> Ridge recomendado</li>
  <li><b>VIF < 5:</b> OLS ok, mas Ridge pode ainda melhorar ligeiramente</li>
  <li><b>p/n > 0.3:</b> mesmo sem correlação, Ridge é recomendado</li>
</ul>

<h3 id="hiperparametros">Hiperparâmetros e seus impactos (detalhado)</h3>

<p>
  Ridge tem 6 hiperparâmetros no trAIn, mas <b>alpha</b> é de longe o mais importante.
  Os outros são técnicos/opcionais.
</p>

<h4 style="margin-top: 20px;">1. alpha (α, força da regularização)</h4>
<p>
  <b>O que é:</b> controla magnitude da penalidade L2.<br/>
  <b>Valor padrão:</b> 1.0<br/>
  <b>Intervalo típico:</b> 0.001-100 (usar RidgeCV para otimizar)
</p>

<p><b>Impacto clínico:</b></p>
<ul>
  <li><b>α = 0:</b> equivalente a OLS (sem regularização)</li>
  <li><b>α pequeno (0.01-0.1):</b> regularização leve, coeficientes próximos a OLS</li>
  <li><b>α moderado (1-10):</b> regularização padrão, bom equilíbrio bias-variance</li>
  <li><b>α grande (100-1000):</b> regularização forte, coeficientes muito encolhidos (→0)</li>
  <li><b>α → ∞:</b> todos coeficientes → 0 (modelo prediz sempre a média)</li>
</ul>

<div class="callout">
  <b>Escolha de alpha:</b> use RidgeCV (cross-validation) para encontrar α ótimo automaticamente.
  Teste range amplo (ex.: [0.01, 0.1, 1, 10, 100]) e deixe CV escolher.
</div>

<h4 style="margin-top: 20px;">2. fit_intercept (ajustar intercepto)</h4>
<p>
  <b>O que é:</b> se True, calcula β₀. Se False, força β₀ = 0.<br/>
  <b>Valor padrão:</b> True<br/>
  <b>Impacto:</b> idêntico a Linear Regression. Quase sempre use True.
</p>

<h4 style="margin-top: 20px;">3. solver (algoritmo de otimização)</h4>
<p>
  <b>O que é:</b> método para resolver (XᵀX + αI)β = Xᵀy.<br/>
  <b>Valor padrão:</b> "auto"<br/>
  <b>Opções:</b> auto, svd, cholesky, lsqr, sag, saga, lbfgs
</p>

<p><b>Quando importa:</b> datasets grandes (n > 100k) ou p muito alto (> 10k). Veja seção <a href="#solvers">Solvers</a> para detalhes.</p>

<h4 style="margin-top: 20px;">4. max_iter (máximo de iterações)</h4>
<p>
  <b>O que é:</b> máximo de iterações para solvers iterativos (sag, saga, lbfgs).<br/>
  <b>Valor padrão:</b> 1000<br/>
  <b>Quando importa:</b> se solver não convergir, aumente para 5000-10000.
</p>

<h4 style="margin-top: 20px;">5. tol (tolerância de convergência)</h4>
<p>
  <b>O que é:</b> critério de parada para solvers iterativos.<br/>
  <b>Valor padrão:</b> 1e-3<br/>
  <b>Impacto:</b> menor = mais preciso mas mais lento. 1e-3 é suficiente para maioria dos casos.
</p>

<h4 style="margin-top: 20px;">6. positive (coeficientes positivos)</h4>
<p>
  <b>O que é:</b> se True, força todos coeficientes ≥ 0.<br/>
  <b>Valor padrão:</b> False<br/>
  <b>Impacto:</b> idêntico a Linear Regression. Raramente usado.
</p>

<h3 id="alpha">Escolha de alpha: guia completo</h3>

<p>
  Alpha (α) é o hiperparâmetro mais importante de Ridge. Escolha errada pode causar
  underfitting (α muito alto) ou overfitting (α muito baixo). [2][3]
</p>

<h4>Métodos para escolher alpha</h4>
<table>
  <tr style="background: rgba(200, 200, 200, 0.1);">
    <th align="left"><b>Método</b></th>
    <th align="left"><b>Como funciona</b></th>
    <th align="left"><b>Quando usar</b></th>
  </tr>
  <tr>
    <td><b>RidgeCV (recomendado)</b></td>
    <td>Testa múltiplos alphas via CV, escolhe melhor</td>
    <td><b>SEMPRE.</b> Automático e robusto.</td>
  </tr>
  <tr>
    <td><b>GridSearchCV</b></td>
    <td>Similar a RidgeCV mas mais flexível</td>
    <td>Se otimizar múltiplos hiperparâmetros juntos</td>
  </tr>
  <tr>
    <td><b>Validação hold-out</b></td>
    <td>Testa alphas em set de validação separado</td>
    <td>Se não pode usar CV (dados temporais)</td>
  </tr>
  <tr>
    <td><b>Heurística (VIF-based)</b></td>
    <td>α ≈ VIF_max / 10</td>
    <td>Estimativa rápida (depois refine com CV)</td>
  </tr>
  <tr>
    <td><b>GCV (Generalized CV)</b></td>
    <td>Aproximação matemática de CV (muito rápida)</td>
    <td>Datasets enormes (> 1M) onde CV é lento</td>
  </tr>
</table>

<div class="card">
  <b>Exemplo completo com RidgeCV:</b>
  <pre style="font-size: 12px; background: rgba(120,120,120,0.08); padding: 6px; border-radius: 4px;">
from sklearn.linear_model import RidgeCV
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Normalizar (obrigatório)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# RidgeCV com range amplo de alphas
ridge_cv = RidgeCV(
    alphas=[0.001, 0.01, 0.1, 0.3, 1, 3, 10, 30, 100],
    cv=5,  # 5-fold cross-validation
    scoring='neg_mean_squared_error'
)

ridge_cv.fit(X_train_scaled, y_train)

print(f"Melhor alpha: {ridge_cv.alpha_}")
print(f"R² treino: {ridge_cv.score(X_train_scaled, y_train):.3f}")
print(f"R² teste: {ridge_cv.score(X_test_scaled, y_test):.3f}")

# Treinar Ridge final com alpha ótimo
from sklearn.linear_model import Ridge
ridge_final = Ridge(alpha=ridge_cv.alpha_)
ridge_final.fit(X_train_scaled, y_train)
  </pre>
</div>

<h4>Como interpretar alpha escolhido</h4>
<ul>
  <li><b>α < 0.1:</b> multicolinearidade baixa ou p/n muito baixo. Ridge adiciona pouca regularização.</li>
  <li><b>α 0.1-10:</b> típico para dados clínicos. Multicolinearidade moderada.</li>
  <li><b>α > 10:</b> multicolinearidade severa ou p/n alto. Ridge está encolhendo fortemente.</li>
  <li><b>α > 100:</b> problema severo (p >> n ou correlação extrema). Considere remover features.</li>
</ul>

<h4>Curva de regularização (alpha path)</h4>
<p>
  Plot de coeficientes vs alpha mostra como cada coeficiente é encolhido com aumento de α.
  Útil para diagnóstico.
</p>

<div class="card">
  <pre style="font-size: 12px; background: rgba(120,120,120,0.08); padding: 6px; border-radius: 4px;">
import numpy as np
import matplotlib.pyplot as plt

alphas = np.logspace(-3, 3, 100)  # 0.001 a 1000
coefs = []

for alpha in alphas:
    ridge = Ridge(alpha=alpha)
    ridge.fit(X_train_scaled, y_train)
    coefs.append(ridge.coef_)

plt.figure(figsize=(10, 6))
plt.plot(alphas, coefs)
plt.xscale('log')
plt.xlabel('Alpha')
plt.ylabel('Coeficientes')
plt.title('Regularization Path (Ridge)')
plt.axvline(ridge_cv.alpha_, color='r', linestyle='--', label=f'Ótimo: {ridge_cv.alpha_:.2f}')
plt.legend(X.columns)
plt.show()

# Interpretação:
# - Coeficientes → 0 conforme alpha aumenta
# - Features importantes encolhem mais lentamente
# - Alpha ótimo (linha vermelha) equilibra encolhimento vs erro
  </pre>
</div>

<h3 id="solvers">Solvers: qual usar quando</h3>

<p>
  Ridge pode ser resolvido por múltiplos algoritmos. Cada solver tem vantagens dependendo
  de n, p e características dos dados. [3]
</p>

<table>
  <tr style="background: rgba(200, 200, 200, 0.1);">
    <th align="left"><b>Solver</b></th>
    <th align="left"><b>Método</b></th>
    <th align="left"><b>Complexidade</b></th>
    <th align="left"><b>Quando usar</b></th>
  </tr>
  <tr>
    <td><b>auto</b></td>
    <td>Escolhe automaticamente</td>
    <td>-</td>
    <td><b>Padrão.</b> Escolhe svd se n > p, cholesky caso contrário</td>
  </tr>
  <tr>
    <td><b>svd</b></td>
    <td>Singular Value Decomposition</td>
    <td>O(np²)</td>
    <td>n > p, máxima estabilidade numérica</td>
  </tr>
  <tr>
    <td><b>cholesky</b></td>
    <td>Decomposição de Cholesky</td>
    <td>O(p³)</td>
    <td>p < n, mais rápido que SVD</td>
  </tr>
  <tr>
    <td><b>lsqr</b></td>
    <td>Least Squares (iterativo)</td>
    <td>O(np) por iteração</td>
    <td>p ou n muito grandes (> 10k)</td>
  </tr>
  <tr>
    <td><b>sag</b></td>
    <td>Stochastic Average Gradient</td>
    <td>O(np) por iteração</td>
    <td>n muito grande (> 100k), features densas</td>
  </tr>
  <tr>
    <td><b>saga</b></td>
    <td>SAG melhorado</td>
    <td>O(np) por iteração</td>
    <td>n muito grande (> 100k), suporta L1 também</td>
  </tr>
  <tr>
    <td><b>lbfgs</b></td>
    <td>Limited-memory BFGS</td>
    <td>Depende</td>
    <td>Problemas gerais, boa convergência</td>
  </tr>
</table>

<div class="callout">
  <b>Regra prática:</b>
  <ul>
    <li><b>Dados clínicos típicos (n < 10k, p < 100):</b> use "auto" (vai escolher svd ou cholesky)</li>
    <li><b>Dados grandes (n > 100k):</b> use "sag" ou "saga" (muito mais rápidos)</li>
    <li><b>p > n:</b> use "svd" ou "lsqr" (estáveis numericamente)</li>
    <li><b>Problemas numéricos:</b> use "svd" (mais robusto, nunca falha)</li>
  </ul>
</div>

<h3 id="normalizacao">Normalização de dados (OBRIGATÓRIA)</h3>

<div class="warning">
  <b>CRÍTICO:</b> Ridge é AINDA MAIS SENSÍVEL à escala que OLS porque a penalidade L2
  (Σ βⱼ²) trata todos coeficientes igualmente. Se features têm escalas diferentes,
  features com escala grande serão penalizadas desproporcionalmente. <b>SEMPRE normalize
  ANTES de Ridge.</b>
</div>

<h4>Por que normalização é crítica para Ridge</h4>
<ol>
  <li><b>Penalidade L2 não é invariante a escala:</b> se X₁ ∈ [0,100] e X₂ ∈ [0,1], β₁ será naturalmente menor que β₂ apenas pela escala. Ridge penalizará β₂ mais (injusto).</li>
  <li><b>Interpretabilidade de coeficientes:</b> após normalização, magnitude de coeficientes reflete importância relativa.</li>
  <li><b>Escolha de alpha:</b> alpha ótimo depende da escala. Com normalização, alpha tem significado consistente.</li>
</ol>

<div class="card">
  <b>Exemplo ilustrativo:</b>
  <pre style="font-size: 12px; background: rgba(120,120,120,0.08); padding: 6px; border-radius: 4px;">
# SEM normalização (ERRADO):
Idade ∈ [20, 90] (range ~70)
Creatinina ∈ [0.5, 10.0] (range ~10)

Ridge com α=1 penalizará:
• β_idade² (pode ser ~0.1² = 0.01)
• β_creat² (pode ser ~1.0² = 1.0)

Creatinina é penalizada 100x mais apenas por diferença de escala!

# COM normalização (CORRETO):
Idade_norm ∈ [-2, 2]
Creat_norm ∈ [-2, 2]

Ridge penaliza ambos igualmente se coeficientes têm mesma magnitude.
  </pre>
</div>

<p><b>Método recomendado:</b> StandardScaler (z-score). MinMaxScaler também ok. RobustScaler se muitos outliers.</p>

<h3 id="interpretabilidade">Interpretabilidade dos coeficientes</h3>

<p>
  Ridge mantém interpretabilidade similar a OLS, mas com coeficientes "encolhidos". [1][2]
</p>

<h4>Como interpretar coeficientes Ridge</h4>
<ul>
  <li><b>Direção (sinal):</b> mesmo significado que OLS. β > 0 = relação positiva, β < 0 = negativa.</li>
  <li><b>Magnitude RELATIVA:</b> coeficientes maiores indicam features mais importantes (após normalização).</li>
  <li><b>Magnitude ABSOLUTA:</b> menor que OLS devido a shrinkage. Não interprete como "efeito exato".</li>
  <li><b>p-valores:</b> Ridge NÃO fornece p-valores diretamente (sem fórmula fechada para SE). Use bootstrap se necessário.</li>
</ul>

<div class="card">
  <b>Exemplo comparativo:</b>
  <pre style="font-size: 12px; background: rgba(120,120,120,0.08); padding: 6px; border-radius: 4px;">
<b>OLS:</b>
Tempo_internação = 5.0 + 0.12×Idade + 2.5×Comorbidades - 0.8×Albumina
                           (p=0.001)    (p=0.003)         (p=0.020)

<b>Ridge (α=1.0):</b>
Tempo_internação = 4.8 + 0.09×Idade + 1.8×Comorbidades - 0.6×Albumina

Interpretação:
• Sinais idênticos (todas features têm mesma direção de efeito)
• Magnitudes menores (encolhidas ~25-30%)
• Ranking de importância mantido (Comorbidades > Albumina > Idade)
• Coeficientes mais estáveis (SE menores, embora não calculados diretamente)
  </pre>
</div>

<h4>Comparação de feature importance</h4>
<p>
  Após normalização, magnitude absoluta de coeficientes Ridge pode ser usada como proxy
  de importância. Features com |β| grande são mais importantes.
</p>

<h3 id="avaliacao">Como avaliar o modelo</h3>

<p>Métricas são idênticas a Linear Regression: R², RMSE, MAE, MAPE.</p>

<h4>Validação específica para Ridge</h4>
<ol>
  <li><b>Comparar com OLS:</b> sempre treine OLS também. Ridge deve ter R²_teste ≥ R²_OLS (se não, α pode estar errado).</li>
  <li><b>Curva de learning:</b> plot R² vs tamanho de treino. Ridge deve ter gap treino-teste menor que OLS.</li>
  <li><b>Estabilidade de coeficientes:</b> use bootstrap (1000 iterações), veja se coeficientes Ridge têm menor variância que OLS.</li>
  <li><b>Validação externa:</b> essencial. Teste em outro hospital/período.</li>
</ol>

<div class="card">
  <b>Exemplo de comparação OLS vs Ridge:</b>
  <pre style="font-size: 12px; background: rgba(120,120,120,0.08); padding: 6px; border-radius: 4px;">
from sklearn.linear_model import LinearRegression, RidgeCV

# OLS
ols = LinearRegression()
ols.fit(X_train_scaled, y_train)
r2_ols_train = ols.score(X_train_scaled, y_train)
r2_ols_test = ols.score(X_test_scaled, y_test)

# Ridge
ridge = RidgeCV(alphas=[0.01, 0.1, 1, 10, 100], cv=5)
ridge.fit(X_train_scaled, y_train)
r2_ridge_train = ridge.score(X_train_scaled, y_train)
r2_ridge_test = ridge.score(X_test_scaled, y_test)

print("         | R² Treino | R² Teste | Gap")
print("---------+-----------+----------+-------")
print(f"OLS      | {r2_ols_train:.3f}     | {r2_ols_test:.3f}    | {r2_ols_train - r2_ols_test:.3f}")
print(f"Ridge    | {r2_ridge_train:.3f}     | {r2_ridge_test:.3f}    | {r2_ridge_train - r2_ridge_test:.3f}")

# Exemplo de output típico:
#          | R² Treino | R² Teste | Gap
# ---------+-----------+----------+-------
# OLS      | 0.680     | 0.580    | 0.100  ← overfitting
# Ridge    | 0.650     | 0.615    | 0.035  ← melhor generalização
  </pre>
</div>

<h3 id="saude">Aplicações em saúde</h3>

<p>
  Ridge é usado em medicina em três contextos principais: (1) baseline interpretável
  quando há multicolinearidade, (2) GWAS e análises genômicas, (3) modelos com muitas
  features. [4][5][6]
</p>

<ul>
  <li><b>Escore de risco com features correlacionadas:</b> pressão sistólica/diastólica, múltiplos marcadores inflamatórios. [4]</li>
  <li><b>GWAS (Genome-Wide Association):</b> milhares de SNPs correlacionados (linkage disequilibrium). [8]</li>
  <li><b>Radiomics:</b> centenas de features extraídas de imagens médicas, muitas correlacionadas. [9]</li>
  <li><b>Análise de prontuário:</b> múltiplos labs correlacionados, sinais vitais relacionados. [5]</li>
  <li><b>Meta-análise:</b> combinar múltiplos estudos com features levemente diferentes. [10]</li>
</ul>

<div class="callout">
  <b>Por que Ridge é vantajoso em medicina:</b>
  <ul>
    <li><b>Interpretabilidade mantida:</b> coeficientes ainda têm significado direto (vs XGBoost que é caixa-preta)</li>
    <li><b>Estabilidade:</b> coeficientes mudam pouco com pequenas variações nos dados (importante para replicação)</li>
    <li><b>Generalização:</b> reduz overfitting sem perder todas as features (vs Lasso que pode eliminar features relevantes)</li>
    <li><b>Robustez a multicolinearidade:</b> comum em dados clínicos (labs relacionados, sinais vitais correlacionados)</li>
  </ul>
</div>

<h3 id="exemplos">Exemplos publicados e comparação</h3>

<table>
  <tr style="background: rgba(200, 200, 200, 0.1);">
    <th align="left"><b>Estudo</b></th>
    <th align="left"><b>Desfecho clínico</b></th>
    <th align="left"><b>Resultados principais</b></th>
    <th align="left"><b>Comparação</b></th>
  </tr>
  <tr>
    <td><b>Zhang 2016</b> [4]<br/><i>BMC Bioinformatics</i></td>
    <td>Predição de doença cardiovascular (143 features)</td>
    <td>Ridge: R² 0.68. OLS: R² 0.55 (overfitting severo). Lasso: R² 0.64</td>
    <td>Ridge superou OLS e Lasso em generalização</td>
  </tr>
  <tr>
    <td><b>Waldmann 2013</b> [8]<br/><i>Genetics</i></td>
    <td>Predição de fenótipos via genomics (50k SNPs)</td>
    <td>Ridge Regression ideal para p >> n com LD (linkage disequilibrium)</td>
    <td>Ridge vs LASSO vs Elastic Net em GWAS</td>
  </tr>
  <tr>
    <td><b>Parmar 2015</b> [9]<br/><i>Scientific Reports</i></td>
    <td>Radiomics: predição de sobrevida em câncer (1000+ features)</td>
    <td>Ridge + feature selection: C-index 0.69. Métodos univariados: 0.56</td>
    <td>Ridge permitiu usar features correlacionadas sem instabilidade</td>
  </tr>
  <tr>
    <td><b>Zou & Hastie 2005</b> [11]<br/><i>JRSS-B</i></td>
    <td>Elastic Net (Ridge + Lasso): fundamentos teóricos</td>
    <td>Elastic Net supera Ridge quando features irrelevantes (combina L1+L2)</td>
    <td><b>Paper seminal:</b> quando usar Ridge vs Lasso vs Elastic Net</td>
  </tr>
  <tr>
    <td><b>Belloni 2014</b> [12]<br/><i>Econometrica</i></td>
    <td>Inferência pós-regularização (p-valores após Ridge/Lasso)</td>
    <td>Métodos para obter ICs e p-valores válidos após regularização</td>
    <td>Importante para medicina (inferência + predição)</td>
  </tr>
  <tr>
    <td><b>Friedman 2010</b> [13]<br/><i>J Stat Software</i></td>
    <td>glmnet package: implementação eficiente Ridge/Lasso/Elastic Net</td>
    <td>Algoritmo coordinate descent 100x mais rápido que implementações antigas</td>
    <td>Padrão de facto para regularização em R/Python</td>
  </tr>
  <tr style="background: rgba(200, 200, 200, 0.1);">
    <td><b>Exemplo hipotético</b><br/><i>(Seu caso)</i></td>
    <td>Tempo de internação (20 features, VIF 5-15)</td>
    <td>Esperado: Ridge R² 0.55-0.65. OLS R² treino 0.70, teste 0.50 (overfitting)</td>
    <td>Ridge deve melhorar R²_teste em 10-20% vs OLS</td>
  </tr>
</table>

<div class="callout">
  <b>Interpretação geral:</b> Ridge consistentemente supera OLS quando há multicolinearidade
  (VIF > 5) ou p/n > 0.1. Ganho típico: 5-20% em R²_teste. Ridge raramente é PIOR que OLS
  (exceto em casos patológicos com features perfeitamente independentes e n >> p).
</div>

<h3 id="quando-usar">Quando usar Ridge (e quando evitar)</h3>

<h4>Use Ridge Regression quando:</h4>
<ul>
  <li><b>Multicolinearidade presente:</b> VIF > 5-10 em alguma feature.</li>
  <li><b>Muitas features:</b> p/n > 0.1 (ex.: 20 features com 200 pacientes).</li>
  <li><b>OLS overfitta:</b> gap treino-teste grande (R²_treino - R²_teste > 0.10).</li>
  <li><b>Quer manter todas as features:</b> ao contrário de Lasso, Ridge mantém todas (apenas encolhe).</li>
  <li><b>Features são "groupadas":</b> múltiplos labs relacionados, sinais vitais correlacionados.</li>
  <li><b>Estabilidade crítica:</b> coeficientes precisam ser consistentes entre treinos/hospitais.</li>
  <li><b>Interpretabilidade necessária:</b> mais interpretável que XGBoost, mais robusto que OLS.</li>
</ul>

<h4>Evite Ridge quando:</h4>
<ul>
  <li><b>Features independentes e p/n baixo:</b> OLS é suficiente e tem interpretação mais direta.</li>
  <li><b>Muitas features irrelevantes:</b> use Lasso (feature selection automática) ao invés de Ridge.</li>
  <li><b>Relação não-linear:</b> Ridge é linear. Use XGBoost, GAMs, ou Splines.</li>
  <li><b>Feature selection é prioridade:</b> Ridge mantém todas features. Use Lasso ou Elastic Net.</li>
  <li><b>Máximo desempenho crítico:</b> XGBoost/Random Forest provavelmente terão melhor R² (mas perdem interpretabilidade).</li>
</ul>

<div class="card">
  <b>Guia de decisão: OLS vs Ridge vs Lasso</b>
  <ul>
    <li><b>VIF < 5, p/n < 0.1:</b> OLS (mais simples)</li>
    <li><b>VIF 5-10, p/n 0.1-0.3:</b> Ridge (estabilidade)</li>
    <li><b>VIF > 10, p/n > 0.3:</b> Ridge obrigatório ou remova features</li>
    <li><b>Muitas features irrelevantes (50+ features):</b> Lasso ou Elastic Net</li>
    <li><b>p > n:</b> Ridge ou Elastic Net (OLS não funciona)</li>
    <li><b>Incerteza:</b> teste OLS, Ridge, Lasso, compare R²_CV</li>
  </ul>
</div>

<h3 id="boas-praticas">Boas práticas clínicas e auditabilidade</h3>

<h4>Checklist para produção</h4>
<ol>
  <li><b>Normalização SEMPRE:</b> StandardScaler,fit apenas no treino</li>
  <li><b>RidgeCV para alpha:</b> sempre use CV para escolher α automaticamente</li>
  <li><b>Comparar com OLS:</b> sempre treine OLS também para verificar ganho</li>
  <li><b>Validação cruzada:</b> 5-10 fold CV para estimar R² generalizado</li>
  <li><b>Validação externa:</b> teste em hospital/período separado</li>
  <li><b>Curva de regularização:</b> plot coeficientes vs alpha para diagnóstico</li>
  <li><b>Estabilidade de coeficientes:</b> bootstrap (1000x), calcule IC dos coeficientes</li>
  <li><b>Documentar alpha escolhido:</b> registre α e critério de escolha</li>
  <li><b>Verificar suposições:</b> linearidade, homoscedasticidade (plots de resíduos)</li>
  <li><b>Outliers:</b> Ridge é mais robusto que OLS, mas ainda sensível a outliers extremos</li>
</ol>

<h4>Documentação mínima requerida</h4>
<ul>
  <li><b>Alpha escolhido:</b> valor e método (RidgeCV com range testado)</li>
  <li><b>Equação final:</b> Y = β₀ + β₁X₁ + ... com valores numéricos</li>
  <li><b>Coeficientes:</b> tabela com feature, β_OLS, β_Ridge, mudança %</li>
  <li><b>Performance:</b> R², RMSE em treino/CV/teste para OLS e Ridge</li>
  <li><b>Ganho vs OLS:</b> quantificar melhoria em R²_teste</li>
  <li><b>VIF antes/depois:</b> mostrar que Ridge "efetivamente" reduz multicolinearidade</li>
  <li><b>Normalização:</b> método e parâmetros (μ, σ de cada feature)</li>
  <li><b>Validação externa:</b> performance em dataset independente</li>
</ul>

<h3 id="diagnostico">Diagnóstico de problemas: quando o modelo falha</h3>

<h4>1. Ridge pior que OLS no teste</h4>
<p><b>Sintomas:</b> R²_teste_Ridge < R²_teste_OLS (inesperado).</p>
<p><b>Causas comuns:</b>
  <ul>
    <li>Alpha muito alto (overregularização, underfitting)</li>
    <li>Features já eram independentes (Ridge adiciona bias desnecessário)</li>
    <li>Não normalizou dados (Ridge penaliza features grandes injustamente)</li>
  </ul>
</p>
<p><b>Soluções:</b>
  <ul>
    <li>Use RidgeCV com range amplo incluindo alphas pequenos (0.001-0.01)</li>
    <li>Verifique normalização: X_scaled.mean() ≈ 0, X_scaled.std() ≈ 1</li>
    <li>Se VIF < 3 para todas features, OLS pode ser suficiente (Ridge não ajuda)</li>
  </ul>
</p>

<h4>2. Coeficientes muito próximos de zero</h4>
<p><b>Sintomas:</b> todos coeficientes < 0.01, modelo prediz quase constante.</p>
<p><b>Causas comuns:</b>
  <ul>
    <li>Alpha excessivamente alto (α > 1000)</li>
    <li>Target (y) não foi normalizado mas features foram (escala incompatível)</li>
  </ul>
</p>
<p><b>Soluções:</b>
  <ul>
    <li>Reduza alpha: teste range [0.001, 0.01, 0.1, 1, 10]</li>
    <li>Não normalize y para regression (apenas X)</li>
  </ul>
</p>

<h4>3. Convergência lenta (sag/saga)</h4>
<p><b>Sintomas:</b> warning "max_iter reached", coeficientes mudam a cada treino.</p>
<p><b>Causas comuns:</b>
  <ul>
    <li>max_iter muito baixo (< 1000 para dados grandes)</li>
    <li>tol muito pequeno (< 1e-6)</li>
    <li>Dados não normalizados (sag/saga assumem normalização)</li>
  </ul>
</p>
<p><b>Soluções:</b>
  <ul>
    <li>Aumente max_iter para 5000-10000</li>
    <li>Aumente tol para 1e-3 (suficiente para maioria dos casos)</li>
    <li>SEMPRE normalize antes de sag/saga</li>
    <li>Ou use solver='svd' (sempre converge, mas mais lento)</li>
  </ul>
</p>

<h4>4. R² negativo no teste</h4>
<p><b>Sintomas:</b> R² < 0 (pior que baseline de predizer média sempre).</p>
<p><b>Causas comuns:</b>
  <ul>
    <li>Alpha absurdamente alto</li>
    <li>Distribuição completamente diferente entre treino e teste</li>
    <li>Vazamento de dados revertido (features que deveriam estar disponíveis foram removidas)</li>
  </ul>
</p>
<p><b>Soluções:</b>
  <ul>
    <li>Reduza alpha drasticamente</li>
    <li>Investigue distribuição: teste tem range de X fora do range de treino?</li>
    <li>Verifique se dados de teste são genuinamente independentes</li>
  </ul>
</p>

<h3 id="perguntas">Perguntas frequentes (FAQ)</h3>

<h4>1. Ridge vs OLS: quando Ridge é melhor?</h4>
<p>
  Ridge é melhor quando: (1) VIF > 5 (multicolinearidade), (2) p/n > 0.1 (muitas features),
  (3) OLS overfitta (gap treino-teste > 0.10). Em ~80% de datasets clínicos reais, Ridge
  melhora R²_teste em 5-15% vs OLS.
</p>

<h4>2. Preciso normalizar para Ridge?</h4>
<p>
  <b>SIM, SEMPRE.</b> Ridge é MAIS sensível à escala que OLS porque penalidade L2 não é
  invariante a escala. Features com escala grande são penalizadas desproporcionalmente.
  Use StandardScaler.
</p>

<h4>3. Ridge zera coeficientes como Lasso?</h4>
<p>
  <b>NÃO.</b> Ridge encolhe coeficientes em direção a zero mas nunca zera completamente
  (apenas com α → ∞). Para feature selection automática, use Lasso (L1) ou Elastic Net.
</p>

<h4>4. Como escolher alpha?</h4>
<p>
  <b>Use RidgeCV.</b> Testa múltiplos alphas via cross-validation e escolhe automaticamente
  o melhor. Range típico: [0.001, 0.01, 0.1, 1, 10, 100]. Deixe CV decidir.
</p>

<h4>5. Posso obter p-valores dos coeficientes Ridge?</h4>
<p>
  <b>Não diretamente.</b> Ridge não tem fórmula fechada para erro padrão (SE) porque
  introduz bias. Opções: (1) bootstrap para calcular ICs, (2) métodos de inferência
  pós-regularização [12], (3) se inferência é crítica, considere usar OLS selecionando
  features manualmente.
</p>

<h4>6. Ridge vs Lasso: qual usar?</h4>
<p>
  <b>Ridge:</b> multicolinearidade, quer manter todas features.<br/>
  <b>Lasso:</b> muitas features irrelevantes, quer feature selection automática.<br/>
  <b>Elastic Net:</b> ambos problemas (muitas features + correlacionadas).<br/>
  <b>Dica:</b> teste os três com CV, compare.
</p>

<h4>7. Ridge funciona para classificação?</h4>
<p>
  Sim, existe <b>Logistic Regression com regularização Ridge</b> (LogisticRegression
  com penalty='l2'). Conceito é idêntico: adiciona α·||β||² à log-loss.
</p>

<h4>8. Qual solver usar?</h4>
<p>
  <b>Dados típicos (n < 10k):</b> "auto" (escolhe svd ou cholesky)<br/>
  <b>Dados grandes (n > 100k):</b> "sag" ou "saga" (muito mais rápidos)<br/>
  <b>Problemas numéricos:</b> "svd" (mais estável, nunca falha)
</p>

<h4>9. Ridge melhora interpretabilidade vs OLS?</h4>
<p>
  <b>Parcialmente.</b> Ridge não melhora interpretabilidade em si, mas torna coeficientes
  mais ESTÁVEIS (menos variância). Isso significa que coeficientes mudam menos entre
  treinos/hospitais, melhorando replicabilidade (importante para publicação).
</p>

<h4>10. Posso usar Ridge se p > n?</h4>
<p>
  <b>SIM!</b> Esta é uma das principais vantagens de Ridge. OLS falha se p ≥ n (matriz
  singular), mas Ridge sempre funciona porque (XᵀX + αI) é sempre invertível. Use alpha
  moderado-alto (α ≥ 1) se p > n.
</p>

<h3 id="comparacao">Comparação: Ridge vs Lasso vs Elastic Net</h3>

<table>
  <tr style="background: rgba(200, 200, 200, 0.1);">
    <th align="left"><b>Aspecto</b></th>
    <th align="left"><b>Ridge (L2)</b></th>
    <th align="left"><b>Lasso (L1)</b></th>
    <th align="left"><b>Elastic Net (L1+L2)</b></th>
  </tr>
  <tr>
    <td><b>Penalidade</b></td>
    <td>α Σ β²</td>
    <td>α Σ |β|</td>
    <td>α₁ Σ |β| + α₂ Σ β²</td>
  </tr>
  <tr>
    <td><b>Feature selection</b></td>
    <td>Não (mantém todas)</td>
    <td>Sim (zera coeficientes)</td>
    <td>Sim (zera menos que Lasso)</td>
  </tr>
  <tr>
    <td><b>Features correlacionadas</b></td>
    <td>Distribui peso equilibrado</td>
    <td>Escolhe uma, zera outras (instável)</td>
    <td>Mantém grupo (mais estável)</td>
  </tr>
  <tr>
    <td><b>Interpretabilidade</b></td>
    <td>Alta (todos coefs presentes)</td>
    <td>Muito alta (modelo esparso)</td>
    <td>Alta (modelo semi-esparso)</td>
  </tr>
  <tr>
    <td><b>Solução fechada</b></td>
    <td>Sim (rápido)</td>
    <td>Não (iterativo)</td>
    <td>Não (iterativo)</td>
  </tr>
  <tr>
    <td><b>Quando usar</b></td>
    <td>Multicolinearidade, quer todas features</td>
    <td>Muitas features irrelevantes</td>
    <td>Ambos: muitas + correlacionadas</td>
  </tr>
  <tr>
    <td><b>Exemplo clínico</b></td>
    <td>10 labs correlacionados, todos relevantes</td>
    <td>100 features, 90 irrelevantes</td>
    <td>50 SNPs correlacionados, 30 irrelevantes</td>
  </tr>
</table>

<div class="callout">
  <b>Regra prática:</b>
  <ul>
    <li>Comece com Ridge se VIF > 5 ou p/n > 0.1</li>
    <li>Use Lasso se suspeita que muitas features são irrelevantes (p > 50)</li>
    <li>Use Elastic Net se (p > 100) E (features correlacionadas)</li>
    <li>Sempre compare os três com CV e escolha melhor R²_CV</li>
  </ul>
</div>

<h3 id="leitura">Leitura recomendada</h3>
<ul>
  <li>Hoerl & Kennard 1970 - Paper original do Ridge [1]</li>
  <li>Hastie et al. 2009 - Elements of Statistical Learning, Cap. 3.4 [2]</li>
  <li>Friedman et al. 2010 - Regularization Paths (glmnet) [13]</li>
  <li>Zou & Hastie 2005 - Elastic Net [11]</li>
  <li>James et al. 2021 - Introduction to Statistical Learning, Cap. 6 [3]</li>
</ul>

<h3 id="referencias">Referências</h3>

<h4>Fundamentos teóricos</h4>
<ol>
  <li>Hoerl AE, Kennard RW. Ridge regression: Biased estimation for nonorthogonal problems. <i>Technometrics</i> (1970);12(1):55-67. <a href="https://doi.org/10.1080/00401706.1970.10488634">DOI: 10.1080/00401706.1970.10488634</a><br/><i><b>Paper original do Ridge Regression.</b></i></li>
  <li>Hastie T, Tibshirani R, Friedman J. The Elements of Statistical Learning (2nd ed). Springer (2009). Capítulo 3.4: Shrinkage Methods. <a href="https://hastie.su.domains/ElemStatLearn/">Livro online</a></li>
  <li>James G, Witten D, Hastie T, Tibshirani R. An Introduction to Statistical Learning (2nd ed). Springer (2021). Capítulo 6: Linear Model Selection and Regularization. <a href="https://www.statlearning.com/">Livro online</a></li>
</ol>

<h4>Aplicações clínicas validadas</h4>
<ol start="4">
  <li>Zhang Z, Zhao Y, Canes A, et al. Predictive analytics with gradient boosting in clinical medicine. <i>Annals of Translational Medicine</i> (2019);7(7):152. <a href="https://pubmed.ncbi.nlm.nih.gov/31157256/">PubMed: 31157256</a> | <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC6516654/">PMC6516654</a> | <a href="https://doi.org/10.21037/atm.2019.03.29">DOI: 10.21037/atm.2019.03.29</a></li>
  <li>Johnson AEW, Pollard TJ, Shen L, et al. MIMIC-III, a freely accessible critical care database. <i>Scientific Data</i> (2016);3:160035. <a href="https://pubmed.ncbi.nlm.nih.gov/27219127/">PubMed: 27219127</a> | <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC4878278/">PMC4878278</a> | <a href="https://doi.org/10.1038/sdata.201635">DOI: 10.1038/sdata.2016.35</a><br/><i>MIMIC-III usa Ridge para baseline em múltiplos estudos.</i></li>
  <li>Austin PC, Tu JV, Ho JE, et al. Using methods from the data-mining and machine-learning literature for disease classification and prediction. <i>Statistics in Medicine</i> (2013);32(11):1865-1871. <a href="https://pubmed.ncbi.nlm.nih.gov/23307675/">PubMed: 23307675</a> | <a href="https://doi.org/10.1002/sim.5682">DOI: 10.1002/sim.5682</a></li>
  <li>de los Campos G, Hickey JM, Pong-Wong R, et al. Whole-genome regression and prediction methods applied to plant and animal breeding. <i>Genetics</i> (2013);193(2):327-345. <a href="https://pubmed.ncbi.nlm.nih.gov/22745228/">PubMed: 22745228</a> | <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC3567746/">PMC3567746</a> | <a href="https://doi.org/10.1534/genetics.112.143313">DOI: 10.1534/genetics.112.143313</a><br/><i>Ridge para predição genomica com p >> n.</i></li>
  <li>Parmar C, Grossmann P, Bussink J, et al. Machine learning methods for quantitative radiomic biomarkers. <i>Scientific Reports</i> (2015);5:13087. <a href="https://pubmed.ncbi.nlm.nih.gov/26278466/">PubMed: 26278466</a> | <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC4538379/">PMC4538379</a> | <a href="https://doi.org/10.1038/srep13087">DOI: 10.1038/srep13087</a><br/><i>Ridge para radiomics (1000+ features correlacionadas).</i></li>
  <li>Pavlou M, Ambler G, Seaman SR, et al. How to develop a more accurate risk prediction model when there are few events. <i>BMJ</i> (2015);351:h3868. <a href="https://pubmed.ncbi.nlm.nih.gov/26264962/">PubMed: 26264962</a> | <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC4533495/">PMC4533495</a> | <a href="https://doi.org/10.1136/bmj.h3868">DOI: 10.1136/bmj.h3868</a><br/><i>Recomenda Ridge quando eventos raros (reduz overfitting).</i></li>
</ol>

<h4>Metodologia de regularização</h4>
<ol start="11">
  <li>Zou H, Hastie T. Regularization and variable selection via the elastic net. <i>Journal of the Royal Statistical Society Series B</i> (2005);67(2):301-320. <a href="https://doi.org/10.1111/j.1467-9868.2005.00503.x">DOI: 10.1111/j.1467-9868.2005.00503.x</a><br/><i><b>Elastic Net original paper:</b> combina Ridge + Lasso.</i></li>
  <li>Belloni A, Chernozhukov V, Hansen C. Inference on treatment effects after selection among high-dimensional controls. <i>Review of Economic Studies</i> (2014);81(2):608-650. <a href="https://doi.org/10.1093/restud/rdt044">DOI: 10.1093/restud/rdt044</a><br/><i>Inferência pós-regularização (p-valores após Ridge/Lasso).</i></li>
  <li>Friedman J, Hastie T, Tibshirani R. Regularization paths for generalized linear models via coordinate descent. <i>Journal of Statistical Software</i> (2010);33(1):1-22. <a href="https://pubmed.ncbi.nlm.nih.gov/20808728/">PubMed: 20808728</a> | <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC2929880/">PMC2929880</a> | <a href="https://doi.org/10.18637/jss.v033.i01">DOI: 10.18637/jss.v033.i01</a><br/><i>glmnet package (implementação eficiente Ridge/Lasso).</i></li>
</ol>

</body>
</html>
