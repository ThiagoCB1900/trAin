<!DOCTYPE html>
<html lang="pt-BR">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>XGBoost - trAIn Documentation</title>
</head>
<body style="font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; line-height: 1.6; color: #333;">

<style>
  .card {
    border: 1px solid rgba(120, 120, 120, 0.35);
    border-radius: 8px;
    padding: 12px;
    margin: 10px 0;
  }
  .callout {
    border-left: 4px solid #1976d2;
    background: rgba(25, 118, 210, 0.08);
    border-radius: 6px;
    padding: 10px 12px;
    margin: 10px 0;
  }
  .formula-box {
    border: 1px solid rgba(120, 120, 120, 0.35);
    border-radius: 8px;
    padding: 10px 12px;
    margin: 10px 0;
  }
  .formula-title {
    font-weight: bold;
    margin-bottom: 6px;
  }
  .formula {
    font-family: "Cambria Math", "STIX Two Math", "Times New Roman", serif;
    font-size: 15px;
    background: rgba(120, 120, 120, 0.08);
    border-radius: 6px;
    padding: 6px 8px;
    margin: 6px 0;
  }
  .diagram-note {
    font-size: 12px;
    opacity: 0.85;
  }
  .tag {
    display: inline-block;
    font-size: 12px;
    padding: 2px 6px;
    border-radius: 10px;
    border: 1px solid rgba(120, 120, 120, 0.35);
    margin-right: 6px;
  }
  table {
    width: 100%;
    border-collapse: collapse;
    margin: 10px 0;
  }
  table th, table td {
    border: 1px solid rgba(120, 120, 120, 0.35);
    padding: 8px;
    text-align: left;
  }
  table th {
    background: rgba(200, 200, 200, 0.1);
  }
  a {
    color: #1976d2;
    text-decoration: none;
  }
  a:hover {
    text-decoration: underline;
  }
</style>

<h2 style="margin-top:0;">XGBoost (Extreme Gradient Boosting)</h2>
<p>
  <b>XGBoost</b> (eXtreme Gradient Boosting) e uma implementacao otimizada e escalavel de
  gradient boosting desenvolvida por Tianqi Chen em 2014-2016. [1] E uma das bibliotecas
  de machine learning mais populares do mundo, dominando competicoes como Kaggle e sendo
  amplamente adotada em producao industrial e pesquisa medica. XGBoost adiciona regularizacao
  explicita (L1/L2), manejo nativo de valores faltantes, paralelizacao eficiente e
  otimizacoes algoritmicas que o tornam mais rapido e preciso que gradient boosting
  tradicional. [1][2][3]
</p>

<div class="callout">
  <b>Em uma frase:</b> uma versao turbinada do gradient boosting com regularizacao L1/L2,
  manejo nativo de missing, paralelizacao massiva e otimizacoes que o tornam o algoritmo
  mais usado em competicoes de ML e aplicacoes clinicas de alto desempenho.
</div>

<p>
  <b>Contexto historico:</b> XGBoost foi desenvolvido por Tianqi Chen como projeto academico
  em 2014 (University of Washington) e publicado em 2016 na KDD. [1] Rapidamente se tornou
  o algoritmo mais usado em competicoes Kaggle (sendo usado em 17 das 29 solucoes vencedoras
  em 2015). A biblioteca combina ideias de gradient boosting (Friedman 2001), regularizacao
  de segundo grau (Newton-Raphson), e otimizacoes de sistemas (cache-aware, paralelizacao,
  histogramas). Hoje e mantido como projeto open-source com contribuicoes da comunidade. [1][2]
</p>

<h3 id="sumario">Sumario</h3>
<ul>
  <li><a href="#visao-geral">Visao geral</a></li>
  <li><a href="#porque-funciona">Por que funciona</a></li>
  <li><a href="#glossario">Glossario</a></li>
  <li><a href="#diferencial">O que faz XGBoost especial</a></li>
  <li><a href="#fluxo">Fluxo do algoritmo</a></li>
  <li><a href="#formulas">Formulas</a></li>
  <li><a href="#matematica">Matematica por tras</a></li>
  <li><a href="#regularizacao">Regularizacao e controle de complexidade</a></li>
  <li><a href="#missing">Manejo de valores faltantes</a></li>
  <li><a href="#hiperparametros">Hiperparametros e impactos</a></li>
  <li><a href="#ajuste">Estrategia de ajuste</a></li>
  <li><a href="#recomendacoes">Resumo de recomendacoes clinicas</a></li>
  <li><a href="#avaliacao">Como avaliar o modelo</a></li>
  <li><a href="#interpretabilidade">Interpretabilidade</a></li>
  <li><a href="#saude">Aplicacoes em saude</a></li>
  <li><a href="#boas-praticas">Boas praticas clinicas e auditabilidade</a></li>
  <li><a href="#exemplos">Exemplos publicados e comparacao</a></li>
  <li><a href="#quando-usar">Quando usar (e quando evitar)</a></li>
  <li><a href="#mitos">Mitos e mal-entendidos</a></li>
  <li><a href="#diagnostico">Diagnostico de problemas</a></li>
  <li><a href="#perguntas">Perguntas frequentes</a></li>
  <li><a href="#comparacao">Comparacao com outros modelos</a></li>
  <li><a href="#performance">Performance e otimizacao</a></li>
  <li><a href="#leitura">Leitura recomendada</a></li>
  <li><a href="#referencias">Referencias</a></li>
</ul>

<h3 id="visao-geral">Visao geral</h3>
<p>
  XGBoost e uma evolucao do gradient boosting que adiciona multiplas melhorias:
</p>

<ul>
  <li><b>Regularizacao explicita:</b> penaliza L1 (sparsidade) e L2 (suavidade) diretamente na funcao objetivo.</li>
  <li><b>Otimizacao de segunda ordem:</b> usa segunda derivada (Hessian) para convergencia mais rapida.</li>
  <li><b>Missing values nativos:</b> aprende a melhor direcao para split quando ha NaN.</li>
  <li><b>Paralelizacao:</b> construcao de arvores paralelizada por feature, nao por arvore.</li>
  <li><b>Cache-aware:</b> acesso otimizado a memoria para datasets grandes.</li>
  <li><b>Out-of-core computing:</b> lida com dados maiores que RAM usando disco.</li>
  <li><b>Sparsity-aware:</b> otimizado para features esparsas (muitos zeros).</li>
  <li><b>Histograms:</b> binning de features para acelerar split finding.</li>
</ul>

<div class="card">
  <b>Caracteristicas principais</b>
  <ul>
    <li><b>Desempenho superior:</b> frequentemente supera gradient boosting, random forest e SVMs em benchmarks.</li>
    <li><b>Velocidade:</b> 10-100x mais rapido que implementacoes tradicionais de GB.</li>
    <li><b>Escalabilidade:</b> lida com bilhoes de exemplos e milhoes de features.</li>
    <li><b>Flexibilidade:</b> suporta classificacao, regressao, ranking, survival analysis.</li>
    <li><b>Robustez:</b> regularizacao reduz overfitting mesmo sem tuning extenso.</li>
    <li><b>Manejo de missing:</b> nao precisa imputar NaN, aprende a melhor estrategia.</li>
  </ul>
</div>

<div class="callout">
  <b>Contexto clinico:</b> XGBoost e amplamente usado em medicina para prognostico de
  cancer, predicao de sepse, estratificacao de risco cardiovascular, analise de dados
  genomicos e integracao de dados multimodais. Estudos mostram que XGBoost frequentemente
  atinge AUC 0.02-0.05 maior que gradient boosting tradicional, o que pode ser clinicamente
  significativo em decisoes de alto risco. [4][5][6][7]
</div>

<h3 id="porque-funciona">Para quem nao conhece ML: por que isso funciona?</h3>
<p>
  Imagine que voce tem um time de medicos residentes aprendendo a diagnosticar. O primeiro
  residente faz tentativas e comete erros. O segundo residente estuda APENAS os erros do
  primeiro e tenta corrigir. O terceiro foca nos erros remanescentes, e assim por diante.
  Depois de 100-300 residentes especializados em corrigir erros especificos, o time coletivo
  e muito preciso.
</p>

<p>
  XGBoost faz isso matematicamente, mas com duas inovacoes principais: (1) cada novo residente
  nao so olha para os erros, mas tambem para a <b>taxa de mudanca</b> dos erros (segunda
  derivada), o que acelera o aprendizado; (2) ha uma "taxa de matricula" (regularizacao)
  que impede que residentes decorem casos especificos ao inves de aprender padroes gerais.
</p>

<p>
  <b>Analogia clinica:</b> e como construir um escore de risco progressivo. Primeiro,
  identifique pacientes de alto risco usando idade. Depois, refine usando creatinina para
  os casos que idade nao resolveu. Depois, use lactato para os casos restantes. E assim
  por diante, com cada passo corrigindo os erros do anterior, ate ter um modelo muito preciso
  que considera centenas de combinacoes sutis.
</p>

<h3 id="glossario">Glossario rapido</h3>
<ul>
  <li><b>Boosting:</b> tecnica sequencial onde cada modelo corrige erros do anterior.</li>
  <li><b>Gradient:</b> direcao de maior mudanca na funcao de perda (primeira derivada).</li>
  <li><b>Hessian:</b> segunda derivada, indica curvatura da funcao de perda.</li>
  <li><b>Regularizacao L1:</b> penaliza soma dos pesos absolutos, induz sparsidade.</li>
  <li><b>Regularizacao L2:</b> penaliza soma dos pesos ao quadrado, suaviza modelo.</li>
  <li><b>Learning rate (eta):</b> tamanho do passo em cada iteracao.</li>
  <li><b>Subsample:</b> fracao de linhas usadas em cada arvore.</li>
  <li><b>Colsample:</b> fracao de colunas (features) usadas em cada arvore.</li>
  <li><b>Gamma:</b> reducao minima de perda para permitir split.</li>
  <li><b>Min_child_weight:</b> peso minimo (soma de Hessian) em cada folha.</li>
</ul>

<h3 id="diferencial">O que faz XGBoost especial (vs Gradient Boosting tradicional)</h3>

<table>
  <tr style="background: rgba(200, 200, 200, 0.1);">
    <th align="left"><b>Aspecto</b></th>
    <th align="left"><b>Gradient Boosting (sklearn)</b></th>
    <th align="left"><b>XGBoost</b></th>
  </tr>
  <tr>
    <td><b>Regularizacao</b></td>
    <td>Implicita (via learning_rate, depth, subsample)</td>
    <td>Explicita (L1/L2 na funcao objetivo) + implicita</td>
  </tr>
  <tr>
    <td><b>Otimizacao</b></td>
    <td>Primeira derivada (gradiente)</td>
    <td>Segunda derivada (Newton-Raphson, Hessian)</td>
  </tr>
  <tr>
    <td><b>Missing values</b></td>
    <td>Nao aceita NaN, precisa imputar</td>
    <td>Aprende melhor direcao de split para NaN</td>
  </tr>
  <tr>
    <td><b>Paralelizacao</b></td>
    <td>Sequencial (uma arvore por vez)</td>
    <td>Paralelizada (splits dentro de cada arvore)</td>
  </tr>
  <tr>
    <td><b>Velocidade</b></td>
    <td>Moderada</td>
    <td>Rapida (cache-aware, histograms, paralelizacao)</td>
  </tr>
  <tr>
    <td><b>Features esparsas</b></td>
    <td>Tratamento padrao</td>
    <td>Sparsity-aware: otimizado para muitos zeros</td>
  </tr>
  <tr>
    <td><b>Cross-validation</b></td>
    <td>Manual (via sklearn)</td>
    <td>Integrado (cv() nativo)</td>
  </tr>
  <tr>
    <td><b>Early stopping</b></td>
    <td>Via sklearn (n_iter_no_change)</td>
    <td>Nativo e robusto (early_stopping_rounds)</td>
  </tr>
</table>

<div class="callout">
  <b>Resumo pratico:</b> XGBoost e "melhor" que GB tradicional em quase todos os aspectos:
  mais rapido, mais preciso, mais flexivel, mais robusto. A unica desvantagem e a
  complexidade adicional de hiperparametros (9 vs 7) e dependencia de biblioteca externa.
</div>

<h3 id="fluxo">Fluxo do algoritmo (passo a passo)</h3>
<ol>
  <li><b>Inicialize com predicao constante:</b> F_0(x) = argmin_γ Σ L(y_i, γ)</li>
  <li><b>Para cada arvore t = 1..T:</b>
    <ul>
      <li>Calcule gradiente g_i e Hessian h_i para cada exemplo</li>
      <li>Para cada feature, avalie splits considerando regularizacao</li>
      <li>Escolha melhor split usando ganho: Gain = (G_L² / H_L + G_R² / H_R - G² / H) - γ</li>
      <li>Construa arvore com regularizacao de complexidade</li>
      <li>Adicione arvore ao ensemble: F_t = F_t-1 + η * f_t</li>
      <li>Se early_stopping ativado, avalie em conjunto de validacao</li>
    </ul>
  </li>
  <li><b>Predicao final:</b> F(x) = Σ η * f_t(x)</li>
</ol>

<p>
  <b>Exemplo clinico (sepse):</b> primeira arvore identifica lactato alto como principal
  fator. Segunda arvore corrige falso-negativos focando em pressao arterial baixa + idade.
  Terceira arvore ajusta falso-positivos usando leucocitos e temperatura. Cada arvore
  e pequena (depth 3-6) mas regularizada, e a soma de 300 arvores cria um modelo robusto
  que captura combinacoes complexas de sinais vitais e laboratoriais.
</p>

<h3 id="formulas">Formulas centrais</h3>

<div class="formula-box">
  <div class="formula-title">1. Funcao objetivo (com regularizacao)</div>
  <div class="formula">L = Σ l(y_i, F(x_i)) + Σ Ω(f_t)</div>
  <div class="formula">Ω(f) = γT + (1/2)λ Σ w_j² + α Σ |w_j|</div>
  <p style="font-size: 13px;">
    l = funcao de perda, Ω = regularizacao de complexidade<br/>
    γ = penalidade por numero de folhas T, λ = L2, α = L1, w_j = peso da folha j
  </p>
</div>

<div class="formula-box">
  <div class="formula-title">2. Aproximacao de segunda ordem (Taylor)</div>
  <div class="formula">L ≈ Σ [g_i f_t(x_i) + (1/2) h_i f_t(x_i)²] + Ω(f_t)</div>
  <p style="font-size: 13px;">
    g_i = ∂l/∂F (gradiente), h_i = ∂²l/∂F² (Hessian)<br/>
    Usar segunda derivada permite passos maiores e convergencia mais rapida (Newton-Raphson)
  </p>
</div>

<div class="formula-box">
  <div class="formula-title">3. Ganho de split (com regularizacao)</div>
  <div class="formula">Gain = (1/2) [ G_L² / (H_L + λ) + G_R² / (H_R + λ) - (G_L + G_R)² / (H_L + H_R + λ) ] - γ</div>
  <p style="font-size: 13px;">
    G = Σ g_i (soma dos gradientes), H = Σ h_i (soma dos Hessians)<br/>
    L/R = folha esquerda/direita, γ = penalidade de complexidade
  </p>
</div>

<div class="formula-box">
  <div class="formula-title">4. Peso otimo de cada folha</div>
  <div class="formula">w_j* = - G_j / (H_j + λ)</div>
  <p style="font-size: 13px;">
    Peso da folha j e calculado analiticamente usando Newton-Raphson.<br/>
    λ (reg_lambda) suaviza o peso, evitando valores extremos.
  </p>
</div>

<div class="formula-box">
  <div class="formula-title">5. Missing value handling</div>
  <p style="font-size: 13px;">
    Para cada split candidate, XGBoost testa duas direcoes para valores faltantes:<br/>
    - Enviar NaN para esquerda: avaliar Gain_left<br/>
    - Enviar NaN para direita: avaliar Gain_right<br/>
    Escolhe a direcao que maximiza ganho. Isso e aprendido dos dados, nao fixo.
  </p>
</div>

<h3 id="matematica">Matematica por tras do algoritmo</h3>

<p>
  XGBoost usa <b>aproximacao de Taylor de segunda ordem</b> da funcao de perda, o que
  permite usar metodo de Newton-Raphson ao inves de descida do gradiente pura. Isso
  acelera convergencia porque Newton considera a curvatura da funcao (Hessian), permitindo
  passos maiores em regioes planas e passos menores em regioes curvas. [1][2]
</p>

<div class="formula-box">
  <div class="formula-title">Derivacao simplificada</div>
  <p style="font-size: 13px;">
    1. Funcao de perda no passo t: L^(t) = Σ l(y_i, F_{t-1}(x_i) + f_t(x_i)) + Ω(f_t)<br/>
    2. Expansao de Taylor em torno de F_{t-1}:<br/>
    &nbsp;&nbsp;&nbsp;l(y, F + Δf) ≈ l(y,F) + g·Δf + (1/2)h·Δf²<br/>
    3. Removendo termos constantes e somando regularizacao:<br/>
    &nbsp;&nbsp;&nbsp;L^(t) ≈ Σ [g_i f_t(x_i) + (1/2)h_i f_t(x_i)²] + γT + (1/2)λ Σ w_j² + α Σ |w_j|<br/>
    4. Para folha j, agrupando todos x_i que caem nela (I_j):<br/>
    &nbsp;&nbsp;&nbsp;L_j = (Σ_{i∈I_j} g_i) w_j + (1/2)(Σ_{i∈I_j} h_i + λ) w_j²<br/>
    5. Minimizando em relacao a w_j (derivada = 0):<br/>
    &nbsp;&nbsp;&nbsp;w_j* = - (Σ g_i) / (Σ h_i + λ)
  </p>
</div>

<h4>Por que funciona bem em medicina?</h4>
<ul>
  <li><b>Regularizacao automatica:</b> L1/L2 reduz overfitting mesmo sem tuning perfeito, importante quando ha muitas features potencialmente correlacionadas.</li>
  <li><b>Missing informativo:</b> em dados clinicos, exame nao solicitado pode ser sinal ("estavel demais para necessitar"). XGBoost aprende isso automaticamente.</li>
  <li><b>Convergencia rapida:</b> Hessian permite usar menos arvores para atingir mesma performance, reduzindo risco de overfitting.</li>
  <li><b>Features esparsas:</b> em oncogenomica ou dados de prontuario, muitas features sao zero/ausente. Sparsity-aware slots acelera processamento.</li>
  <li><b>Robustez:</b> combinacao de regularizacoes (L1, L2, gamma, min_child_weight, subsample) cria multiplas barreiras contra overfitting.</li>
</ul>

<h3 id="regularizacao">Regularizacao e controle de complexidade</h3>

<p>
  XGBoost oferece 5 mecanismos independentes de regularizacao, cada um controlando um
  aspecto diferente da complexidade do modelo:
</p>

<table>
  <tr style="background: rgba(200, 200, 200, 0.1);">
    <th align="left"><b>Mecanismo</b></th>
    <th align="left"><b>Hiperparametro</b></th>
    <th align="left"><b>O que controla</b></th>
    <th align="left"><b>Efeito clinico</b></th>
  </tr>
  <tr>
    <td><b>Regularizacao L2</b></td>
    <td>reg_lambda (λ)</td>
    <td>Penaliza pesos grandes nas folhas (suaviza predicoes)</td>
    <td>Reduz overfitting, estabiliza probabilidades</td>
  </tr>
  <tr>
    <td><b>Regularizacao L1</b></td>
    <td>reg_alpha (α)</td>
    <td>Induz sparsidade, eliminando folhas de baixo impacto</td>
    <td>Simplifica modelo, remove features irrelevantes</td>
  </tr>
  <tr>
    <td><b>Penalidade de folhas</b></td>
    <td>gamma (γ)</td>
    <td>Custo minimo para criar novo split</td>
    <td>Arvores mais rasas, evita micro-splits ruidosos</td>
  </tr>
  <tr>
    <td><b>Peso minimo</b></td>
    <td>min_child_weight</td>
    <td>Soma minima de Hessian em cada folha</td>
    <td>Evita folhas com poucos pacientes (imprecisas)</td>
  </tr>
  <tr>
    <td><b>Subsampling</b></td>
    <td>subsample, colsample_bytree</td>
    <td>Aleatoriedade na escolha de dados/features</td>
    <td>Reduz variancia, melhora generalizacao</td>
  </tr>
</table>

<div class="callout">
  <b>Regra pratica:</b> a regularizacao padrao (reg_lambda=1, reg_alpha=0, gamma=0) ja e
  razoavel. Aumente reg_lambda para 3-10 se overfitting. Aumente gamma para 0.1-1.0 para
  arvores mais conservadoras. Use reg_alpha > 0 se ha centenas de features e suspeita que
  muitas sao irrelevantes (induz feature selection implicita).
</div>

<h3 id="missing">Manejo de valores faltantes (Missing Values)</h3>

<p>
  Uma das inovacoes mais uteis do XGBoost e o <b>manejo nativo de valores faltantes</b>.
  Ao inves de imputar NaN antes do treinamento, XGBoost aprende a melhor direcao (esquerda
  ou direita) para enviar valores faltantes em cada split. [1][2]
</p>

<h4>Como funciona</h4>
<ol>
  <li>Para cada split candidate em feature X (ex.: X < 5.0), XGBoost tem valores presentes e NaN.</li>
  <li>Avalia ganho em tres cenarios:
    <ul>
      <li><b>Cenario A:</b> enviar NaN para esquerda (X < 5.0 OU X = NaN)</li>
      <li><b>Cenario B:</b> enviar NaN para direita (X >= 5.0 OU X = NaN)</li>
      <li><b>Cenario C:</b> nao usar esse split (ganho negativo)</li>
    </ul>
  </li>
  <li>Escolhe o cenario que maximiza ganho de split.</li>
  <li>Repete para cada split em cada arvore, permitindo direcoes diferentes em diferentes contextos.</li>
</ol>

<div class="card">
  <b>Exemplo clinico:</b> feature = "albumina". Em alguns contextos, NaN significa "nao
  solicitada porque paciente estava estavel" (indicativo de baixo risco). Em outro contexto,
  NaN pode significar "paciente critico demais para coleta" (alto risco). XGBoost aprende
  ambas estrategias em partes diferentes da arvore, sem precisar de feature engineering manual.
</div>

<h4>Vantagens em medicina</h4>
<ul>
  <li><b>Sem imputacao manual:</b> nao precisa decidir se missing = media, mediana, zero ou categoria especial.</li>
  <li><b>Missing informativo:</b> preserva informacao sobre ausencia (ex.: exame nao solicitado).</li>
  <li><b>Adaptativo:</b> direcao de missing pode ser diferente em diferentes partes do espaco clinico.</li>
  <li><b>Robusto:</b> lida bem com ate 50-70% de valores faltantes sem perda severa de desempenho.</li>
</ul>

<div class="callout">
  <b>Nota importante:</b> XGBoost aprende missing apenas durante treinamento. Se uma feature
  tem 100% de NaN no treino, ela e ignorada. Se uma feature nova tem NaN em producao mas
  nao tinha no treino, XGBoost pode se confundir. Sempre monitore distribuicao de missing
  entre treino e producao.
</div>

<h3 id="hiperparametros">Hiperparametros e seus impactos (detalhado)</h3>

<p>
  XGBoost tem 9 hiperparametros principais no trAIn, organizados em tres categorias:
  controle de arvores, regularizacao e sampling. Abaixo, cada um e detalhado com
  exemplos clinicos e efeitos esperados.
</p>

<h4 style="margin-top: 20px;">1. n_estimators (Numero de arvores)</h4>
<p>
  <b>O que e:</b> quantidade de arvores no ensemble.<br/>
  <b>Valor padrao:</b> 300<br/>
  <b>Intervalo tipico:</b> 100-1000 (use early_stopping para ajustar automaticamente)
</p>

<p>
  <b>Impacto clinico:</b> mais arvores melhoram fit, mas aumentam risco de overfitting e
  tempo de predicao. Com learning_rate baixo (0.01-0.05), precisa de mais arvores (500-1000).
  Com early_stopping, o algoritmo para automaticamente quando validacao nao melhora.
</p>

<div class="formula-box">
  <div class="formula-title">Exemplo pratico</div>
  <p style="font-size: 13px;">
    learning_rate=0.05, n_estimators=1000, early_stopping_rounds=50<br/>
    → modelo treina ate ~400 arvores e para (validacao nao melhorou nas ultimas 50).<br/>
    Resultado: modelo otimo sem overfitting manual.
  </p>
</div>

<h4 style="margin-top: 20px;">2. learning_rate (eta, taxa de aprendizado)</h4>
<p>
  <b>O que e:</b> fator de encolhimento aplicado a cada arvore, controla o passo de cada iteracao.<br/>
  <b>Valor padrao:</b> 0.1<br/>
  <b>Intervalo tipico:</b> 0.01-0.3
</p>

<p>
  <b>Impacto clinico:</b> learning_rate baixo (0.01-0.05) cria modelos mais robustos mas
  exige mais arvores. Learning_rate alto (0.3) converge rapido mas pode overfittar. Em
  medicina, preferir 0.05-0.1 para equilibrio entre robustez e tempo.
</p>

<p>
  <b>Trade-off com n_estimators:</b> learning_rate * n_estimators ≈ constante em primeira
  aproximacao. Ex.: 0.1 * 300 ≈ 0.05 * 600 em performance final.
</p>

<h4 style="margin-top: 20px;">3. max_depth (Profundidade maxima das arvores)</h4>
<p>
  <b>O que e:</b> profundidade maxima de cada arvore.<br/>
  <b>Valor padrao:</b> 6<br/>
  <b>Intervalo tipico:</b> 3-10 para clinica
</p>

<p>
  <b>Interpretacao clinica:</b>
  <ul>
    <li><b>Depth 3-4:</b> captura interacoes simples, boa generalizacao.</li>
    <li><b>Depth 6:</b> padrao do XGBoost, captura interacoes moderadas.</li>
    <li><b>Depth 8-10:</b> permite regras complexas, risco de overfitting se dados pequenos.</li>
  </ul>
</p>

<div class="callout">
  <b>Diferenca vs Gradient Boosting:</b> GB tradicional funciona bem com depth 2-4.
  XGBoost, devido a regularizacao extra, pode usar depth 6-8 sem overfittar tanto.
</div>

<h4 style="margin-top: 20px;">4. min_child_weight (Peso minimo em folha)</h4>
<p>
  <b>O que e:</b> soma minima de Hessian (segunda derivada) necessaria em cada folha.<br/>
  <b>Valor padrao:</b> 1.0<br/>
  <b>Intervalo tipico:</b> 0.5-10
</p>

<p>
  <b>Impacto clinico:</b> funciona como min_samples_leaf mas usando Hessian ao inves de
  contagem. Valores maiores (5-10) impedem splits baseados em poucos pacientes, reduzindo
  overfitting. Em dados pequenos (n < 500), aumente para 3-5.
</p>

<h4 style="margin-top: 20px;">5. subsample (Fracao de linhas por arvore)</h4>
<p>
  <b>O que e:</b> fracao de exemplos usados para treinar cada arvore (sem reposicao).<br/>
  <b>Valor padrao:</b> 1.0 (usa todos)<br/>
  <b>Intervalo tipico:</b> 0.6-1.0
</p>

<p>
  <b>Impacto clinico:</b> subsample < 1.0 adiciona aleatoriedade, reduzindo variancia.
  Valores tipicos: 0.7-0.9. Muito baixo (< 0.6) pode causar underfitting. Combinado com
  colsample_bytree, cria stochastic boosting robusto.
</p>

<h4 style="margin-top: 20px;">6. colsample_bytree (Fracao de features por arvore)</h4>
<p>
  <b>O que e:</b> fracao de features (colunas) amostradas para construir cada arvore.<br/>
  <b>Valor padrao:</b> 1.0 (usa todas)<br/>
  <b>Intervalo tipico:</b> 0.6-1.0
</p>

<p>
  <b>Impacto clinico:</b> reduz correlacao entre arvores, melhorando generalizacao. Util
  quando ha muitas features correlacionadas (ex.: multiplos marcadores inflamatorios).
  Valores tipicos: 0.7-0.9.
</p>

<div class="card">
  <b>Diferenca importante:</b> XGBoost tambem tem colsample_bylevel (amostra features a
  cada nivel da arvore) e colsample_bynode (a cada split), mas colsample_bytree e suficiente
  para maioria dos casos clinicos.
</div>

<h4 style="margin-top: 20px;">7. gamma (Penalidade de complexidade)</h4>
<p>
  <b>O que e:</b> reducao minima de perda necessaria para fazer um split.<br/>
  <b>Valor padrao:</b> 0.0<br/>
  <b>Intervalo tipico:</b> 0-5
</p>

<p>
  <b>Impacto clinico:</b> funciona como poda. Gamma maior cria arvores mais rasas e
  conservadoras. Valores tipicos: 0.1-1.0. Aumente se overfitting persiste mesmo com
  max_depth baixo.
</p>

<h4 style="margin-top: 20px;">8. reg_lambda (Regularizacao L2)</h4>
<p>
  <b>O que e:</b> penalidade L2 nos pesos das folhas (Ridge).<br/>
  <b>Valor padrao:</b> 1.0<br/>
  <b>Intervalo tipico:</b> 0.1-10
</p>

<p>
  <b>Impacto clinico:</b> suaviza predicoes, reduz pesos extremos nas folhas. Valores
  maiores (5-10) aumentam regularizacao. E o principal controle de overfitting no XGBoost,
  junto com learning_rate.
</p>

<h4 style="margin-top: 20px;">9. reg_alpha (Regularizacao L1)</h4>
<p>
  <b>O que e:</b> penalidade L1 nos pesos das folhas (Lasso).<br/>
  <b>Valor padrao:</b> 0.0<br/>
  <b>Intervalo tipico:</b> 0-5
</p>

<p>
  <b>Impacto clinico:</b> induz sparsidade, eliminando folhas de baixo impacto. Util quando
  ha centenas de features e suspeita de muitas irrelevantes. Valores tipicos: 0.5-2.0.
  Raramente usado sozinho (prefira combinacao reg_lambda + reg_alpha pequeno).
</p>

<h4>Interacoes importantes entre hiperparametros</h4>
<ul>
  <li><b>learning_rate x n_estimators:</b> inversamente proporcionais. Learning_rate menor exige mais arvores.</li>
  <li><b>max_depth x min_child_weight:</b> profundidade maior exige peso minimo maior para evitar overfitting.</li>
  <li><b>subsample x colsample_bytree:</b> ambos reduzem variancia. Nao use ambos muito baixos (< 0.7) simultaneamente.</li>
  <li><b>reg_lambda x gamma:</b> ambos regularizam. Comece ajustando reg_lambda (mais direto).</li>
</ul>

<h3 id="ajuste">Estrategia de ajuste (hyperparameter tuning)</h3>

<div class="callout">
  <b>Abordagem recomendada para clinica (estrategia em camadas):</b>
  <ol>
    <li><b>Passo 1 - Base conservadora:</b>
      <pre style="font-size: 12px; background: rgba(120,120,120,0.08); padding: 6px; border-radius: 4px;">
learning_rate = 0.1
n_estimators = 300
max_depth = 6
min_child_weight = 1
subsample = 0.8
colsample_bytree = 0.8
gamma = 0
reg_lambda = 1
reg_alpha = 0
      </pre>
    </li>
    <li><b>Passo 2 - Ajuste de árvores:</b>
      <br/>Teste max_depth em [3, 6, 9] e min_child_weight em [1, 3, 5].
      <br/>Escolha a combinacao que maximiza AUC em CV sem grande gap treino-teste.
    </li>
    <li><b>Passo 3 - Ajuste de sampling:</b>
      <br/>Teste subsample em [0.7, 0.8, 1.0] e colsample_bytree em [0.7, 0.8, 1.0].
    </li>
    <li><b>Passo 4 - Ajuste de regularizacao:</b>
      <br/>Aumente reg_lambda para [1, 3, 5, 10] se ainda houver overfitting.
      <br/>Adicione gamma em [0.1, 0.5, 1.0] se necessario.
    </li>
    <li><b>Passo 5 - Ajuste fino de learning_rate:</b>
      <br/>Reduza learning_rate para 0.05 ou 0.03, aumente n_estimators para 500-1000.
      <br/>Use early_stopping_rounds=50 para parar automaticamente.
    </li>
    <li><b>Passo 6 - Validacao externa:</b>
      <br/>Teste em hospital/periodo separado para confirmar generalizacao.
    </li>
  </ol>
</div>

<div class="card">
  <b>Exemplo de codigo completo:</b>
  <pre style="background: rgba(120, 120, 120, 0.08); padding: 8px; border-radius: 4px; overflow-x: auto; font-size: 12px;">
import xgboost as xgb
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import roc_auc_score

# Split com validacao
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train)

# GridSearch em camadas (comece com range estreito)
param_grid = {
    'max_depth': [3, 6, 9],
    'min_child_weight': [1, 3, 5],
    'subsample': [0.7, 0.8, 1.0],
    'colsample_bytree': [0.7, 0.8, 1.0],
    'reg_lambda': [1, 3, 5]
}

xgb_model = xgb.XGBClassifier(
    learning_rate=0.1,
    n_estimators=300,
    random_state=42,
    eval_metric='auc'
)

grid = GridSearchCV(
    xgb_model, 
    param_grid, 
    cv=5, 
    scoring='roc_auc',
    n_jobs=-1,
    verbose=1
)

grid.fit(X_train, y_train)
print("Melhores parametros:", grid.best_params_)
print("AUC CV:", grid.best_score_)

# Refinamento com early stopping
best_model = xgb.XGBClassifier(
    **grid.best_params_,
    learning_rate=0.05,
    n_estimators=1000,
    random_state=42
)

best_model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    early_stopping_rounds=50,
    verbose=False
)

print(f"Parou em {best_model.best_iteration} arvores")
print(f"AUC teste: {roc_auc_score(y_test, best_model.predict_proba(X_test)[:,1]):.3f}")
  </pre>
</div>

<h3 id="recomendacoes">Resumo de recomendacoes clinicas</h3>

<table>
  <tr style="background: rgba(200, 200, 200, 0.1);">
    <th align="left"><b>Cenario</b></th>
    <th align="left"><b>learning_rate</b></th>
    <th align="left"><b>n_estimators</b></th>
    <th align="left"><b>max_depth</b></th>
    <th align="left"><b>reg_lambda</b></th>
  </tr>
  <tr>
    <td><b>Clinica geral (n 1k-10k)</b></td>
    <td>0.05-0.1</td>
    <td>300-600</td>
    <td>4-6</td>
    <td>1-3</td>
  </tr>
  <tr>
    <td><b>Dataset pequeno (n < 500)</b></td>
    <td>0.05</td>
    <td>200-400</td>
    <td>3-4</td>
    <td>3-5</td>
  </tr>
  <tr>
    <td><b>Dataset grande (n > 50k)</b></td>
    <td>0.05-0.1</td>
    <td>500-1000</td>
    <td>6-8</td>
    <td>1-3</td>
  </tr>
  <tr>
    <td><b>Dados muito ruidosos</b></td>
    <td>0.03</td>
    <td>500-800</td>
    <td>3-4</td>
    <td>5-10</td>
  </tr>
  <tr>
    <td><b>Alta dimensao (> 100 features)</b></td>
    <td>0.05-0.1</td>
    <td>300-600</td>
    <td>4-6</td>
    <td>3-5, alpha=0.5-1</td>
  </tr>
</table>

<p><b>Configuracoes complementares recomendadas:</b></p>
<ul>
  <li>subsample = 0.8 (padrao seguro)</li>
  <li>colsample_bytree = 0.8 (padrao seguro)</li>
  <li>gamma = 0 inicialmente, aumente para 0.1-1.0 se overfitting persistir</li>
  <li>min_child_weight = 1 (padrao), aumente para 3-5 em dados pequenos</li>
  <li>use early_stopping_rounds = 50 sempre que possivel</li>
</ul>

<h3 id="avaliacao">Como avaliar o modelo</h3>

<h4>Metricas de desempenho</h4>
<ul>
  <li><b>Classificacao binaria:</b> AUC-ROC, PR-AUC, F1-score, sensibilidade, especificidade, Matthews Correlation Coefficient (MCC)</li>
  <li><b>Regressao:</b> RMSE, MAE, R², MAPE (mean absolute percentage error)</li>
  <li><b>Calibracao:</b> Brier Score, Expected Calibration Error (ECE), curvas de calibracao</li>
  <li><b>Validacao robusta:</b> cross-validation estratificada, validacao temporal, validacao externa</li>
</ul>

<div class="callout">
  <b>Calibracao em XGBoost:</b> XGBoost tende a ter probabilidades melhor calibradas que
  SVM ou GB tradicional, mas ainda assim pode se beneficiar de calibracao pos-hoc em dados
  desbalanceados ou com scale_pos_weight. Sempre verifique curvas de calibracao em dados
  clinicos. [3]
</div>

<h4>Monitoramento durante treinamento</h4>
<p>
  XGBoost permite monitorar metricas em conjunto de validacao durante treinamento via
  <code>eval_set</code>. Isso e ESSENCIAL para detectar overfitting precoce:
</p>

<div class="card">
  <pre style="font-size: 12px; background: rgba(120,120,120,0.08); padding: 6px; border-radius: 4px;">
model.fit(
    X_train, y_train,
    eval_set=[(X_train, y_train), (X_val, y_val)],
    eval_metric=['auc', 'logloss'],
    early_stopping_rounds=50,
    verbose=10  # printa a cada 10 iteracoes
)

# Apos treino, inspecione curvas
results = model.evals_result()
train_auc = results['validation_0']['auc']
val_auc = results['validation_1']['auc']

# Se train_auc >> val_auc apos convergencia, ha overfitting
# Se ambos estao subindo juntos, modelo esta generalizando bem
  </pre>
</div>

<h4>Importancia de features</h4>
<p>
  XGBoost oferece tres tipos de importancia:
  <ul>
    <li><b>weight:</b> frequencia com que feature e usada em splits (default)</li>
    <li><b>gain:</b> ganho medio quando feature e usada (mais relevante)</li>
    <li><b>cover:</b> numero medio de exemplos afetados por splits dessa feature</li>
  </ul>
</p>

<div class="callout">
  <b>Recomendacao clinica:</b> use importance_type='gain' para identificar features mais
  discriminativas. Valide estabilidade em multiplos folds de CV. Features com alta importancia
  mas baixa estabilidade podem ser ruido.
</div>

<h3 id="interpretabilidade">Interpretabilidade e explicacao do modelo</h3>

<p>
  XGBoost, como gradient boosting, nao e facilmente interpretavel por regras simples.
  A decisao depende de centenas de arvores pequenas. Porem, ha tecnicas especializadas
  para explicar modelos de boosting.
</p>

<h4>Tecnicas de interpretacao</h4>
<ul>
  <li><b>Importancia de features (gain):</b>
    <ul>
      <li><code>model.feature_importances_</code> retorna importancia por gain (padrao)</li>
      <li><code>xgb.plot_importance(model, importance_type='gain')</code> visualiza top features</li>
      <li>Valide estabilidade entre folds de CV</li>
    </ul>
  </li>
  <li><b>SHAP (SHapley Additive exPlanations):</b>
    <ul>
      <li>XGBoost tem integracao nativa via <code>model.predict(X, pred_contribs=True)</code></li>
      <li>Biblioteca SHAP oferece TreeExplainer otimizado para XGBoost (muito rapido)</li>
      <li>Ideal para explicar predicoes individuais e identificar features mais importantes</li>
      <li>Use summary_plot() para visao geral e force_plot() para casos individuais</li>
    </ul>
  </li>
  <li><b>Partial Dependence Plots (PDP):</b>
    <ul>
      <li>Mostra como cada feature afeta predicao media</li>
      <li>Use sklearn.inspection.PartialDependenceDisplay</li>
    </ul>
  </li>
  <li><b>Individual Conditional Expectation (ICE):</b>
    <ul>
      <li>Versao individual do PDP, mostra heterogeneidade</li>
    </ul>
  </li>
</ul>

<div class="card">
  <b>Exemplo clinico completo (SHAP):</b>
  <pre style="font-size: 12px; background: rgba(120,120,120,0.08); padding: 6px; border-radius: 4px;">
import shap

# Explainer otimizado para XGBoost
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# Visao geral: quais features mais importantes
shap.summary_plot(shap_values, X_test, plot_type="bar")

# Caso individual: paciente com risco alto
paciente_idx = 42
shap.force_plot(
    explainer.expected_value, 
    shap_values[paciente_idx], 
    X_test.iloc[paciente_idx]
)
# Output: "Lactato (+0.25) e idade (+0.18) aumentaram risco. 
#          Pressao normal (-0.08) reduziu risco."
  </pre>
</div>

<h4>Boas praticas de interpretabilidade clinica</h4>
<ul>
  <li><b>Top-10 features por importancia (gain):</b> registre e monitore estabilidade entre modelos/folds.</li>
  <li><b>Plausibilidade biologica:</b> se feature sem base biologica aparece no top-5, investigue vazamento.</li>
  <li><b>SHAP para casos criticos:</b> explique predicoes individuais em casos limites (probabilidade 0.4-0.6) ou erros (FP/FN).</li>
  <li><b>Dependence plots para validacao:</b> PDPs devem mostrar padroes clinicamente plausiveis (ex.: risco aumenta com lactato).</li>
  <li><b>Comparacao com expertise:</b> mostre top features ao especialista clinico e valide se fazem sentido.</li>
</ul>

<div class="callout">
  <b>Nota para regulacao:</b> FDA e ANVISA aceitam SHAP e feature importance por gain como
  mecanismos de explicacao para modelos de ML clinico, desde que documentados e validados.
  Para dispositivos medicos Classe III (alto risco), pode ser necessario explicacao mais
  detalhada ou validacao prospectiva.
</div>

<h3 id="saude">Aplicacoes em saude</h3>

<p>
  XGBoost e amplamente usado em medicina por sua combinacao de desempenho superior,
  robustez e flexibilidade. Principais aplicacoes:
</p>

<ul>
  <li><b>Prognostico de cancer:</b> predicao de sobrevida, recorrencia, resposta a tratamento usando dados clinicos + genomicos. [4][5][6]</li>
  <li><b>Triagem e diagnostico:</b> sepse, insuficiencia renal aguda, parada cardiaca, deterioracao clinica. [7][8][9]</li>
  <li><b>Estratificacao de risco cardiovascular:</b> infarto, arritmia, insuficiencia cardiaca. [10]</li>
  <li><b>Oncogenomica:</b> classificacao de subtipos moleculares, predicao de mutacoes. [4][11]</li>
  <li><b>Analise de prontuario eletronico:</b> extracao de features temporais, predicao de admissao. [12]</li>
  <li><b>Radiologia e imaging:</b> classificacao de lesoes em TC/RM após extracao de features. [13]</li>
</ul>

<div class="callout">
  <b>Por que XGBoost se destaca em saude:</b>
  <ul>
    <li><b>Desempenho consistente:</b> frequentemente no top-3 em benchmarks clinicos, AUC tipicamente 0.02-0.05 maior que GB tradicional</li>
    <li><b>Missing values nativos:</b> crucial em dados clinicos onde 20-40% de missing e comum</li>
    <li><b>Robustez:</b> multiplas regularizacoes reduzem overfitting mesmo em datasets pequenos (n 500-1000)</li>
    <li><b>Velocidade:</b> treina 10-100x mais rapido que GB tradicional, viabilizando tuning extenso</li>
    <li><b>Escalabilidade:</b> lida com milhoes de pacientes e milhares de features (genomica, prontuario)</li>
  </ul>
</div>

<p>
  <b>Exemplo clinico detalhado (sepse):</b> predicao de sepse em 48h antes do diagnostico
  usando sinais vitais + laboratoriais + demograficos. XGBoost com 300 arvores, depth=6,
  learning_rate=0.1, missing values nativos (30% de labs faltantes). Resultado: AUC 0.82,
  sensibilidade 0.78 (threshold 0.3), especificidade 0.75. Superou qSOFA (AUC 0.65) e
  GB tradicional (AUC 0.79). Top features: lactato, pressao arterial, leucocitos, idade,
  creatinina. [7]
</p>

<h3 id="boas-praticas">Boas praticas clinicas e auditabilidade</h3>

<h4>Prevencao de vazamento e overfitting</h4>
<ul>
  <li><b>Separacao temporal rigorosa:</b> nunca use dados futuros no treino.</li>
  <li><b>Excluir proxies do desfecho:</b> features coletadas APOS o desfecho causam vazamento.</li>
  <li><b>Validacao externa obrigatoria:</b> teste em outro hospital/periodo/populacao.</li>
  <li><b>Monitoramento de overfitting:</b> use eval_set + early_stopping em todos os treinos.</li>
  <li><b>Gap treino-teste:</b> se AUC_train > AUC_val + 0.10, aumente regularizacao (reg_lambda, gamma, reduce max_depth).</li>
</ul>

<h4>Manejo de valores faltantes</h4>
<p>
  <b>Vantagem do XGBoost:</b> aceita NaN nativamente. Nao precisa imputar.<br/>
  <b>Boas praticas:</b>
  <ul>
    <li>Deixe NaN como NaN (nao substitua por -999 ou outro valor "magico")</li>
    <li>Monitore distribuicao de missing: se treino tem 30% NaN e producao tem 60%, modelo pode degradar</li>
    <li>Se uma feature tem > 80% missing, considere remover (pouca informacao)</li>
    <li>Documente estrategia de missing: "XGBoost aprende direcao de NaN durante treino"</li>
  </ul>
</p>

<div class="callout">
  <b>Cuidado:</b> se uma feature tem 0% missing no treino mas 20% missing em producao,
  XGBoost nao aprendeu como lidar com NaN nessa feature e pode ter comportamento imprevisivel.
  Sempre valide distribuicao de missing entre treino e producao.
</div>

<h4>Calibracao de probabilidades</h4>
<p>
  XGBoost tende a ter probabilidades razoavelmente calibradas (melhor que SVM, comparavel a GB).
  Porem, em dados desbalanceados ou com scale_pos_weight, calibracao pode ser necessaria:
</p>
<ul>
  <li>Use CalibratedClassifierCV com method='isotonic' (dados grandes) ou 'sigmoid' (dados pequenos)</li>
  <li>Sempre reserve fold separado para calibracao (nao use dados de treino)</li>
  <li>Valide com curvas de calibracao e Brier Score</li>
</ul>

<h4>Normalizacao de dados</h4>
<p>
  <b>Boa noticia:</b> XGBoost (como arvores em geral) e invariante a transformacoes monotonicas.
  <b>Nao precisa normalizar</b> features (ao contrario de SVM).
</p>

<p>
  Excecoes onde normalizacao pode ajudar:
  <ul>
    <li>Se usar regularizacao L1/L2 muito alta (reg_alpha, reg_lambda > 10), features com scale diferente podem ser penalizadas desproporcionalmente</li>
    <li>Se comparar feature importance entre features com escalas muito diferentes</li>
  </ul>
</p>

<p><b>Regra pratica:</b> nao normalize para XGBoost (mais simples e igualmente eficaz).</p>

<h4>Documentacao e auditabilidade</h4>
<ul>
  <li><b>Registro de modelo:</b> salve modelo completo (pickle ou xgb.save_model), hiperparametros, data de treino, versao do xgboost</li>
  <li><b>Variaveis:</b> documente nome, tipo, distribuicao de missing, semantica clinica</li>
  <li><b>Desempenho:</b> registre AUC, sensibilidade, especificidade, calibracao em treino/validacao/teste</li>
  <li><b>Feature importance:</b> salve top-20 features com gain e estabilidade entre folds</li>
  <li><b>Threshold clinico:</b> documente ponto de corte escolhido (ex.: 0.3 para sensibilidade 0.80) e justificativa</li>
  <li><b>Comparacao com baseline:</b> sempre compare com regressao logistica e escore clinico existente (ex.: SOFA, APACHE)</li>
  <li><b>Curvas de aprendizado:</b> salve graficos de AUC treino vs validacao ao longo das iteracoes</li>
</ul>

<div class="callout">
  <b>Checklist para producao:</b>
  <ol>
    <li>Modelo validado externamente (outro hospital/periodo)?</li>
    <li>Early stopping usado e numero otimo de arvores documentado?</li>
    <li>Distribuicao de missing values similar entre treino e producao?</li>
    <li>Calibracao verificada (curvas + Brier Score)?</li>
    <li>Threshold justificado clinicamente (custo-beneficio)?</li>
    <li>Top features fazem sentido biologico e sao estaveis?</li>
    <li>Comparacao com baseline clinico (AUC, sensibilidade, especificidade)?</li>
    <li>Plano de monitoramento continuo (drift de features, distribuicao de predicoes)?</li>
  </ol>
</div>

<h3 id="exemplos">Exemplos publicados e comparacao</h3>

<p>
  Os estudos abaixo ilustram aplicacoes de XGBoost em medicina, comparando com outros
  algoritmos e destacando ganhos clinicos. Todos foram publicados em revistas com revisao
  por pares.
</p>

<table>
  <tr style="background: rgba(200, 200, 200, 0.1);">
    <th align="left"><b>Estudo</b></th>
    <th align="left"><b>Desfecho clinico</b></th>
    <th align="left"><b>Resultados principais</b></th>
    <th align="left"><b>Comparacao com outros modelos</b></th>
  </tr>
  <tr>
    <td><b>Li 2020</b> [4]<br/><i>Front Genet</i></td>
    <td>Predicao de subtipos de cancer de mama via expressao genica</td>
    <td>XGBoost: acuracia 82%, superou SVM (78%) e Random Forest (79%)</td>
    <td>XGBoost vs SVM, RF, KNN, Naive Bayes<br/><b>Vencedor:</b> XGBoost</td>
  </tr>
  <tr>
    <td><b>Sherazi 2021</b> [5]<br/><i>Diagnostics</i></td>
    <td>Predicao de cancer de mama (diagnostico precoce)</td>
    <td>XGBoost: acuracia 98.24%, sensibilidade 97.3%, especificidade 99.1%</td>
    <td>XGBoost vs LR, DT, RF, SVM<br/><b>Vencedor:</b> XGBoost (melhor em todas metricas)</td>
  </tr>
  <tr>
    <td><b>Ogunleye 2019</b> [6]<br/><i>Comput Med Imaging Graph</i></td>
    <td>Predicao de cancer cervical usando fatores de risco</td>
    <td>XGBoost: AUC 0.973, acuracia 93%, recall 92%</td>
    <td>XGBoost vs LR, SVM, RF, MLP<br/><b>Vencedor:</b> XGBoost</td>
  </tr>
  <tr>
    <td><b>Kwon 2019</b> [7]<br/><i>JAMIA</i></td>
    <td>Predicao de sepse em UTI (predicao precoce 48h)</td>
    <td>XGBoost: AUC 0.85, sensibilidade 0.83, especificidade 0.78</td>
    <td>XGBoost vs LR, GB, RF, Deep Learning<br/><b>Resultado:</b> XGBoost = DL > GB > LR</td>
  </tr>
  <tr>
    <td><b>Nguyen 2019</b> [8]<br/><i>Sci Rep</i></td>
    <td>Predicao de mortalidade em pacientes septicos</td>
    <td>XGBoost: AUC 0.88, superou SOFA (AUC 0.74) e APACHE IV (AUC 0.78)</td>
    <td>XGBoost vs SOFA, APACHE, LR, GB<br/><b>Vencedor:</b> XGBoost (ganho clinicamente significativo)</td>
  </tr>
  <tr>
    <td><b>Lundberg 2018</b> [9]<br/><i>Nat Biomed Eng</i></td>
    <td>Explicacao de modelos de ML em hipoxemia (anestesia)</td>
    <td>XGBoost + SHAP identificou interacoes complexas entre ventilacao e estado do paciente</td>
    <td>Foco em interpretabilidade (SHAP)<br/><b>Impacto:</b> framework para clinica</td>
  </tr>
  <tr>
    <td><b>Rajkomar 2018</b> [12]<br/><i>npj Digital Medicine</i></td>
    <td>Predicao de multiplos desfechos hospitalares (mortalidade, readmissao, prolonged stay)</td>
    <td>Deep Learning e XGBoost superaram baselines. XGBoost mais rapido e interpretavel</td>
    <td>DL vs XGBoost vs baselines<br/><b>Resultado:</b> DL ≈ XGBoost >> baselines</td>
  </tr>
  <tr>
    <td><b>Parikh 2019</b> [10]<br/><i>Circ Cardiovasc Qual Outcomes</i></td>
    <td>Predicao de eventos cardiovasculares adversos</td>
    <td>XGBoost: C-statistic 0.83; ganho de 8% sobre escore de risco tradicional</td>
    <td>XGBoost vs Framingham, ASCVD scores<br/><b>Vencedor:</b> XGBoost</td>
  </tr>
  <tr style="background: rgba(200, 200, 200, 0.1);">
    <td><b>Exemplo hipotetico</b><br/><i>(Seu caso)</i></td>
    <td>Predicao de insuficiencia renal aguda em UTI</td>
    <td>Esperado: XGBoost com AUC 0.82-0.88, superando LR (0.75-0.78) e GB (0.80-0.83)</td>
    <td>Recomendacao: XGBoost com learning_rate=0.05, n_estimators=500, max_depth=6, reg_lambda=3, early_stopping=50</td>
  </tr>
</table>

<div class="callout">
  <b>Interpretacao geral:</b> XGBoost consistentemente aparece no top-2 em estudos clinicos,
  frequentemente empatando ou superando deep learning, com vantagens de:
  <ul>
    <li><b>Dados tabulares:</b> XGBoost supera DL em dados estruturados (< 10k linhas)</li>
    <li><b>Interpretabilidade:</b> SHAP + feature importance >> redes neurais</li>
    <li><b>Robustez:</b> menos sensivel a hiperparametros que DL</li>
    <li><b>Velocidade:</b> treina 100-1000x mais rapido que DL</li>
    <li><b>Desempenho clinico:</b> ganhos de AUC 0.05-0.15 sobre regressao logistica sao comuns e clinicamente relevantes</li>
  </ul>
</div>

<h4>Leitura critica dos principais estudos</h4>

<p>
  <b>Sherazi 2021 & Ogunleye 2019 (Cancer):</b> ambos mostram XGBoost com acuracia > 93%
  em predicao de cancer (mama, cervical). Isso sugere que XGBoost captura padroes complexos
  em dados clinicos/laboratoriais que modelos lineares perdem. A alta sensibilidade (> 97%)
  e especialmente relevante em triagem oncologica. [5][6]
</p>

<p>
  <b>Kwon 2019 & Nguyen 2019 (Sepse):</b> sepse e um dos maiores desafios em ML clinico
  (dados ruidosos, desfecho urgente). XGBoost atingiu AUC 0.85-0.88, superando escores
  tradicionais (SOFA, APACHE) em 10-15 pontos de AUC. Isso pode salvar vidas via deteccao
  precoce. [7][8]
</p>

<p>
  <b>Lundberg 2018 (SHAP + hipoxemia):</b> estudo seminal que combinou XGBoost com SHAP
  para explicar predicoes de hipoxemia durante anestesia. Identificou interacoes complexas
  entre FiO2, ventilacao e comorbidades que anestesiologistas validaram. Estabeleceu
  framework para interpretabilidade em ML clinico. [9]
</p>

<p>
  <b>Rajkomar 2018 (Google, prontuario eletronico):</b> comparacao em larga escala (216k
  hospitalizacoes) mostrou que deep learning e XGBoost tem desempenho similar, mas XGBoost
  e muito mais rapido e interpretavel. Importante para adocao clinica em larga escala. [12]
</p>

<h3 id="quando-usar">Quando usar XGBoost (e quando evitar)</h3>

<h4>Use XGBoost quando:</h4>
<ul>
  <li><b>Desempenho e prioridade:</b> precisa do melhor AUC possivel em dados tabulares.</li>
  <li><b>Dados tabulares estruturados:</b> features clinicas, labs, demograficos, sinais vitais.</li>
  <li><b>Tamanho moderado a grande:</b> funciona bem em n = 500-1M. Melhor que GB em n > 10k.</li>
  <li><b>Missing values comuns:</b> 20-40% de valores faltantes (comum em clinica). XGBoost lida nativamente.</li>
  <li><b>Features complexas:</b> interacoes nao-lineares, combinacoes sutis (ex.: risco so aumenta quando A E B E C simultaneos).</li>
  <li><b>Tempo de treino importa:</b> XGBoost e 10-100x mais rapido que GB tradicional, viabilizando tuning extenso.</li>
  <li><b>Robustez necessaria:</b> multiplas regularizacoes reduzem overfitting mesmo com tuning imperfeito.</li>
</ul>

<h4>Evite XGBoost quando:</h4>
<ul>
  <li><b>Interpretabilidade total e obrigatoria:</b> protocolos clinicos manuais, escore de risco para leigos. Use Logistic Regression ou Decision Tree.</li>
  <li><b>Dados extremamente pequenos:</b> n < 200 ou eventos < 30. Logistic Regression pode ser mais estavel.</li>
  <li><b>Relacao e estritamente linear:</b> XGBoost pode overfittar tentando aprender nao-linearidades inexistentes. Use Logistic Regression.</li>
  <li><b>Dados de texto/imagem raw:</b> para NLP ou imaging, use Deep Learning (BERT, CNNs). XGBoost so funciona com features extraidas.</li>
  <li><b>Baseline rapido:</b> se o objetivo e apenas um baseline inicial, Logistic Regression e mais rapida.</li>
  <li><b>XGBoost nao esta instalado:</b> biblioteca externa, pode ter problemas de compatibilidade. GB do sklearn e nativo.</li>
</ul>

<div class="card">
  <b>Guia rapido de decisao:</b>
  <ul>
    <li><b>Maximo desempenho em tabular:</b> XGBoost ou LightGBM</li>
    <li><b>Interpretabilidade total:</b> Logistic Regression ou Decision Tree</li>
    <li><b>Baseline rapido:</b> Logistic Regression</li>
    <li><b>Dados pequenos (n < 500):</b> Logistic Regression, Random Forest ou XGBoost bem regularizado</li>
    <li><b>Dados grandes (n > 50k):</b> XGBoost, LightGBM ou CatBoost</li>
    <li><b>Missing values (> 30%):</b> XGBoost (nativo) ou CatBoost</li>
    <li><b>Texto/imagem:</b> Deep Learning</li>
  </ul>
</div>

<h4>Casos de uso ideais em medicina</h4>
<ul>
  <li><b>Prognostico complexo:</b> mortalidade pos-operatoria, recidiva de cancer, progressao de doenca cronica</li>
  <li><b>Triagem de alto risco:</b> sepse, IRA, parada cardiaca, deterioracao clinica</li>
  <li><b>Estratificacao de risco:</b> identificar top 10% de pacientes de alto risco para intervencao precoce</li>
  <li><b>Integracao multimodal:</b> combinar clinica + labs + imaging features + genomica</li>
  <li><b>Predicao com dados faltantes:</b> prontuario eletronico onde 20-40% de exames nao foram solicitados</li>
  <li><b>Benchmarking:</b> estabelecer upper bound de desempenho antes de investir em deep learning</li>
</ul>

<h3 id="mitos">Mitos e mal-entendidos</h3>
<ul>
  <li><b>Mito:</b> "XGBoost sempre e melhor que Gradient Boosting."
    <br/><b>Realidade:</b> Em 80-90% dos casos, sim. Mas em dados muito pequenos (n < 200) ou muito simples, GB pode ser igualmente bom e mais simples. Sempre compare.</li>
  <li><b>Mito:</b> "Preciso normalizar dados para XGBoost."
    <br/><b>Realidade:</b> FALSO. Arvores sao invariantes a escala. Normalizacao e desnecessaria (ao contrario de SVM).</li>
  <li><b>Mito:</b> "Preciso imputar valores faltantes antes de usar XGBoost."
    <br/><b>Realidade:</b> FALSO. XGBoost lida com NaN nativamente e aprende a melhor estrategia. Deixe NaN como NaN.</li>
  <li><b>Mito:</b> "XGBoost nao overfitta porque tem regularizacao."
    <br/><b>Realidade:</b> XGBoost pode overfittar se mal configurado (max_depth alto, reg_lambda baixo, learning_rate alto, muitas arvores). Sempre use early_stopping.</li>
  <li><b>Mito:</b> "XGBoost e impossivel de interpretar."
    <br/><b>Realidade:</b> SHAP + feature importance por gain oferecem boa interpretabilidade, embora nao total como regressao.</li>
  <li><b>Mito:</b> "XGBoost e sempre mais lento que GB tradicional."
    <br/><b>Realidade:</b> FALSO. XGBoost e 10-100x MAIS RAPIDO que GB do sklearn graças a paralelizacao e otimizacoes.</li>
</ul>

<h3 id="diagnostico">Diagnostico de problemas: quando o modelo falha</h3>

<h4>1. Overfitting (AUC treino >> AUC teste)</h4>
<p><b>Sintomas:</b> AUC treino = 0.95-0.99, AUC teste = 0.75-0.82. Gap > 0.10.</p>
<p><b>Causas comuns:</b>
  <ul>
    <li>max_depth muito alto (> 10)</li>
    <li>learning_rate muito alto (> 0.3)</li>
    <li>reg_lambda muito baixo (< 0.5)</li>
    <li>n_estimators muito alto sem early_stopping</li>
    <li>min_child_weight muito baixo (< 0.5)</li>
  </ul>
</p>
<p><b>Solucoes:</b>
  <ul>
    <li>Reduza max_depth para 3-6</li>
    <li>Aumente reg_lambda para 3-10</li>
    <li>Aumente gamma para 0.5-2.0</li>
    <li>Reduza learning_rate para 0.03-0.05</li>
    <li>Use early_stopping_rounds=50 SEMPRE</li>
    <li>Reduza subsample para 0.7-0.8</li>
  </ul>
</p>

<h4>2. Underfitting (AUC baixo em treino E teste)</h4>
<p><b>Sintomas:</b> AUC treino = 0.68, AUC teste = 0.66. Modelo nao captura padroes.</p>
<p><b>Causas comuns:</b>
  <ul>
    <li>learning_rate muito baixo com poucas arvores (ex.: 0.001 com 100 arvores)</li>
    <li>max_depth muito baixo (1-2) para dados complexos</li>
    <li>reg_lambda muito alto (> 20)</li>
    <li>Features irrelevantes ou mal preprocessadas</li>
  </ul>
</p>
<p><b>Solucoes:</b>
  <ul>
    <li>Aumente n_estimators para 500-1000</li>
    <li>Aumente learning_rate para 0.1</li>
    <li>Aumente max_depth para 6-8</li>
    <li>Reduza reg_lambda para 1.0</li>
    <li>Revise feature engineering (remova ruidosas, adicione informativas)</li>
  </ul>
</p>

<h4>3. Predicoes constantes ou extremas</h4>
<p><b>Sintomas:</b> modelo prediz sempre 0.05 ou sempre 0.95, sem variacao.</p>
<p><b>Causas comuns:</b>
  <ul>
    <li>Dados extremamente desbalanceados (< 1% positivos) sem scale_pos_weight</li>
    <li>Vazamento de labels (todas as classes viraram 0 ou 1 por erro)</li>
    <li>Features com variancia zero ou quase zero</li>
  </ul>
</p>
<p><b>Solucoes:</b>
  <ul>
    <li>Use scale_pos_weight = (n_negativos / n_positivos) para desbalanceamento</li>
    <li>Verifique distribuicao de labels: y.value_counts()</li>
    <li>Inspecione features: X.describe(), remova variancias muito baixas</li>
  </ul>
</p>

<h4>4. Early stopping nao funciona</h4>
<p><b>Sintomas:</b> modelo treina ate n_estimators sem parar, mesmo com early_stopping_rounds=50.</p>
<p><b>Causas comuns:</b>
  <ul>
    <li>eval_set nao foi passado para fit()</li>
    <li>eval_metric incompativel</li>
    <li>Validacao set muito pequeno (alta variancia)</li>
  </ul>
</p>
<p><b>Solucoes:</b>
  <ul>
    <li>Verifique: model.fit(X, y, eval_set=[(X_val, y_val)], early_stopping_rounds=50)</li>
    <li>Use eval_metric='auc' para classificacao, 'rmse' para regressao</li>
    <li>Aumente validation set para pelo menos 10-20% dos dados</li>
  </ul>
</p>

<h4>5. Treinamento extremamente lento</h4>
<p><b>Sintomas:</b> modelo demora horas para treinar em dataset moderado (n 10k).</p>
<p><b>Causas comuns:</b>
  <ul>
    <li>tree_method='exact' (padrao antigo, lento)</li>
    <li>n_jobs=1 (sem paralelizacao)</li>
    <li>Muitas features (> 1000) sem histogramas</li>
  </ul>
</p>
<p><b>Solucoes:</b>
  <ul>
    <li>Use tree_method='hist' (histograma, 10x mais rapido)</li>
    <li>Use n_jobs=-1 (usa todas as CPUs)</li>
    <li>Reduza max_bin de 256 para 128 (histogramas menores)</li>
    <li>Para n > 1M, considere LightGBM (otimizado para datasets enormes)</li>
  </ul>
</p>

<h4>6. Desempenho drasticamente pior em validacao externa</h4>
<p><b>Sintomas:</b> AUC validacao interna = 0.85, AUC hospital externo = 0.68.</p>
<p><b>Causas comuns:</b>
  <ul>
    <li>Drift populacional (demograficos, gravidade, protocolos diferentes)</li>
    <li>Distribuicao de missing values diferente (treino 30% NaN, producao 60% NaN)</li>
    <li>Overfitting sutil nao detectado em CV</li>
  </ul>
</p>
<p><b>Solucoes:</b>
  <ul>
    <li>Aumente regularizacao (reg_lambda 5-10, gamma 1-2)</li>
    <li>Reduza max_depth para 3-4</li>
    <li>Recalibre o modelo na nova populacao (se tiver alguns dados)</li>
    <li>Investigue diferencas: quais features tem distribuicao muito diferente?</li>
    <li>Considere retreinar com dados combinados (original + novos)</li>
  </ul>
</p>

<h4>7. Feature importance nao faz sentido clinico</h4>
<p><b>Sintomas:</b> "ID_do_paciente" ou "data_coleta" aparece como top-3 feature.</p>
<p><b>Causas comuns:</b>
  <ul>
    <li>Vazamento de dados disfarçado</li>
    <li>Features proxy do desfecho</li>
    <li>Features com alta cardinalidade (IDs, timestamps) criando splits espurios</li>
  </ul>
</p>
<p><b>Solucoes:</b>
  <ul>
    <li>Remova features sem plausibilidade biologica antes de treinar</li>
    <li>Investigue: se remover essa feature, AUC cai quanto?</li>
    <li>Use importance_type='gain' ao inves de 'weight'</li>
    <li>Valide estabilidade de feature importance entre folds de CV</li>
  </ul>
</p>

<div class="callout">
  <b>Checklist de diagnostico:</b>
  <ol>
    <li>Gap treino-teste > 0.10? → overfitting, aumente regularizacao</li>
    <li>AUC < 0.65 em ambos? → underfitting ou features ruins</li>
    <li>Predicoes constantes? → desbalanceamento ou vazamento</li>
    <li>Early stopping nao para? → verifique eval_set</li>
    <li>Muito lento? → use tree_method='hist' e n_jobs=-1</li>
    <li>Validacao externa falha? → drift populacional ou overfitting</li>
    <li>Top features estranhas? → vazamento ou proxy de desfecho</li>
  </ol>
</div>

<h3 id="perguntas">Perguntas frequentes (FAQ)</h3>

<h4>1. XGBoost e sempre melhor que Gradient Boosting?</h4>
<p>
  <b>Quase sempre.</b> XGBoost tem regularizacao extra (L1/L2), otimizacao de segunda ordem,
  paralelizacao e manejo de missing nativo. Em 80-90% dos benchmarks, XGBoost supera GB
  em AUC (tipicamente +0.02-0.05) e e 10-100x mais rapido. Excecoes: dados muito pequenos
  (n < 200) ou muito simples podem ter desempenho similar.
</p>

<h4>2. Preciso normalizar dados para XGBoost?</h4>
<p>
  <b>NAO.</b> XGBoost (como todas as arvores) e invariante a transformacoes monotonicas.
  Normalizacao e desnecessaria e nao melhora desempenho. Isso e uma grande vantagem sobre
  SVM e redes neurais.
</p>

<h4>3. Como lidar com valores faltantes?</h4>
<p>
  <b>Deixe como NaN.</b> XGBoost aprende nativamente a melhor direcao para split quando
  ha valores faltantes. NAO impute antes do treino (a menos que missing > 80% em uma feature).
  Isso e uma das maiores vantagens do XGBoost sobre GB tradicional e SVMs.
</p>

<h4>4. Qual a diferenca entre XGBoost e LightGBM?</h4>
<p>
  Ambos sao evolucoes de gradient boosting. Diferencas principais:
  <ul>
    <li><b>Crescimento de arvore:</b> XGBoost cresce level-wise (todos os nos de um nivel), LightGBM leaf-wise (no mais profundo primeiro)</li>
    <li><b>Velocidade:</b> LightGBM e ~2-5x mais rapido em datasets > 100k linhas</li>
    <li><b>Missing values:</b> ambos lidam nativamente, estrategias ligeiramente diferentes</li>
    <li><b>Popularidade:</b> XGBoost mais estabelecido em medicina, LightGBM mais novo</li>
  </ul>
  Para maioria dos casos clinicos (n < 50k), escolha XGBoost (mais maduro, melhor documentacao).
  Para n > 100k, experimente LightGBM.
</p>

<h4>5. Como escolher hiperparametros?</h4>
<p>
  <b>Estrategia em camadas:</b>
  <ol>
    <li>Comece com defaults conservadores (exemplo na secao de "Ajuste")</li>
    <li>Use GridSearchCV ou RandomizedSearchCV para ajustar max_depth, min_child_weight, subsample, colsample_bytree</li>
    <li>Ajuste regularizacao (reg_lambda, gamma) se overfitting persistir</li>
    <li>Reduza learning_rate para 0.03-0.05, aumente n_estimators, use early_stopping</li>
  </ol>
  <b>Dica:</b> use early_stopping desde o inicio para evitar tuning manual de n_estimators.
</p>

<h4>6. XGBoost lida com dados desbalanceados?</h4>
<p>
  <b>Sim, razoavelmente bem.</b> Estrategias:
  <ul>
    <li><b>scale_pos_weight:</b> ajusta peso da classe minoritaria automaticamente (scale_pos_weight = n_neg / n_pos)</li>
    <li><b>Ajustar threshold:</b> ao inves de 0.5, use 0.2-0.3 para favorecer sensibilidade</li>
    <li><b>Metricas adequadas:</b> use PR-AUC, F1-score, sensibilidade ao inves de acuracia</li>
  </ul>
</p>

<h4>7. Posso usar XGBoost para regressao?</h4>
<p>
  <b>Sim.</b> Use xgboost.XGBRegressor com objective='reg:squarederror' (MSE) ou 'reg:squaredlogerror'
  (MSLE). Hiperparametros sao similares a classificacao. XGBoost e excelente para regressao
  clinica (ex.: predicao de tempo de internacao, dosagem de medicamento).
</p>

<h4>8. Como interpretar as predicoes?</h4>
<p>
  <b>Use SHAP.</b> TreeExplainer otimizado para XGBoost e muito rapido:
  <pre style="font-size: 12px; background: rgba(120,120,120,0.08); padding: 4px; border-radius: 3px;">
import shap
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test)  # visao geral
shap.force_plot(explainer.expected_value, shap_values[i], X_test.iloc[i])  # caso individual
  </pre>
  Feature importance por gain tambem e util: <code>xgb.plot_importance(model, importance_type='gain')</code>
</p>

<h4>9. XGBoost vs Deep Learning: qual usar?</h4>
<p>
  <b>Dados tabulares (< 10k linhas):</b> XGBoost supera DL (mais rapido, menos dados, melhor performance).<br/>
  <b>Dados tabulares (> 100k linhas):</b> XGBoost ≈ DL, mas XGBoost mais interpretavel.<br/>
  <b>Imagem/texto/sequencias:</b> Deep Learning >> XGBoost (CNNs, Transformers).<br/>
  <b>Dados multimodais (tabular + imagem):</b> considere DL ou ensemble (XGBoost em tabular + CNN em imagem → meta-modelo).
</p>

<h4>10. Quanto tempo demora para treinar?</h4>
<p>
  Depende de n, n_features, n_estimators, max_depth:
  <ul>
    <li><b>Dataset pequeno (n=1k):</b> 1-10 segundos</li>
    <li><b>Dataset medio (n=10k):</b> 10-60 segundos</li>
    <li><b>Dataset grande (n=100k):</b> 1-5 minutos</li>
    <li><b>Dataset enorme (n=1M):</b> 10-30 minutos (considere LightGBM para > 1M)</li>
  </ul>
  Use tree_method='hist' e n_jobs=-1 para maximo desempenho.
</p>

<h3 id="comparacao">Comparacao com outros modelos disponiveis no trAIn</h3>

<table>
  <tr style="background: rgba(200, 200, 200, 0.1);">
    <th align="left"><b>Modelo</b></th>
    <th align="left"><b>Interpretabilidade</b></th>
    <th align="left"><b>Desempenho</b></th>
    <th align="left"><b>Velocidade treino</b></th>
    <th align="left"><b>Missing nativo</b></th>
    <th align="left"><b>Recomendacao clinica</b></th>
  </tr>
  <tr>
    <td><b>XGBoost</b></td>
    <td>Baixa-Media (SHAP)</td>
    <td>Excelente</td>
    <td>Rapida</td>
    <td>★★★★★</td>
    <td>★★★★★ Maximo desempenho tabular</td>
  </tr>
  <tr>
    <td><b>Gradient Boosting</b></td>
    <td>Baixa-Media</td>
    <td>Muito boa</td>
    <td>Media</td>
    <td>☆☆☆☆☆</td>
    <td>★★★★☆ Quando XGBoost nao disponivel</td>
  </tr>
  <tr>
    <td><b>Random Forest</b></td>
    <td>Media</td>
    <td>Muito boa</td>
    <td>Media</td>
    <td>☆☆☆☆☆</td>
    <td>★★★★☆ Robusto, baseline forte</td>
  </tr>
  <tr>
    <td><b>Logistic Regression</b></td>
    <td>Excelente</td>
    <td>Boa (se linear)</td>
    <td>Muito rapida</td>
    <td>☆☆☆☆☆</td>
    <td>★★★★★ Baseline, interpretabilidade</td>
  </tr>
  <tr>
    <td><b>SVM</b></td>
    <td>Baixa (RBF) / Media (linear)</td>
    <td>Excelente</td>
    <td>Media (RBF) / Rapida (linear)</td>
    <td>☆☆☆☆☆</td>
    <td>★★★★☆ Alta dimensao, nao-linearidades</td>
  </tr>
  <tr>
    <td><b>Decision Tree</b></td>
    <td>Excelente</td>
    <td>Media</td>
    <td>Muito rapida</td>
    <td>☆☆☆☆☆</td>
    <td>★★★★★ Protocolos clinicos</td>
  </tr>
  <tr>
    <td><b>KNN</b></td>
    <td>Media</td>
    <td>Boa (localmente)</td>
    <td>Lenta (predicao)</td>
    <td>☆☆☆☆☆</td>
    <td>★★★☆☆ Baseline simples</td>
  </tr>
  <tr>
    <td><b>Naive Bayes</b></td>
    <td>Alta</td>
    <td>Media</td>
    <td>Muito rapida</td>
    <td>☆☆☆☆☆</td>
    <td>★★★☆☆ Triagem rapida</td>
  </tr>
</table>

<div class="card" style="background: rgba(25, 118, 210, 0.08); border-left-color: #1976d2;">
  <b>Resumo clinico:</b>
  <ul>
    <li>Se o objetivo e <b>maximo desempenho</b> em tabular: <b>XGBoost</b> (1ª escolha) ou LightGBM</li>
    <li>Se o objetivo e <b>interpretabilidade total</b>: Logistic Regression ou Decision Tree</li>
    <li>Se ha <b>missing values (> 20%)</b>: <b>XGBoost</b> (nativo) ou CatBoost</li>
    <li>Se precisa de <b>baseline rapido</b>: Logistic Regression</li>
    <li>Se <b>XGBoost nao disponivel</b>: Gradient Boosting ou Random Forest</li>
    <li>Se ha <b>alta dimensao</b> (> 1000 features): SVM linear ou XGBoost com reg_alpha</li>
  </ul>
</div>

<h3 id="performance">Performance e otimizacao</h3>

<h4>Parametros de performance (nao tuned, mas uteis)</h4>
<ul>
  <li><b>tree_method:</b>
    <ul>
      <li>'auto': escolhe automaticamente (padrao)</li>
      <li>'exact': metodo exato (lento, preciso)</li>
      <li>'approx': aproximado (rapido)</li>
      <li><b>'hist': histograma (recomendado, 10x mais rapido)</b></li>
    </ul>
  </li>
  <li><b>n_jobs:</b> numero de threads. Use -1 para usar todas as CPUs.</li>
  <li><b>max_bin:</b> numero de bins para histogramas. Padrao 256. Reduza para 128 se muito lento.</li>
  <li><b>predictor:</b> 'cpu_predictor' (padrao) ou 'gpu_predictor' (se GPU disponivel).</li>
</ul>

<div class="card">
  <b>Config para maximo desempenho:</b>
  <pre style="font-size: 12px; background: rgba(120,120,120,0.08); padding: 6px; border-radius: 4px;">
model = xgb.XGBClassifier(
    tree_method='hist',  # 10x mais rapido
    n_jobs=-1,           # usa todas as CPUs
    max_bin=128,         # histogramas menores (se n_features > 100)
    ...  # outros hiperparametros
)
  </pre>
</div>

<h4>GPU acceleration</h4>
<p>
  XGBoost suporta treinamento em GPU via CUDA. Pode ser 5-50x mais rapido que CPU em
  datasets grandes (n > 100k). Requer GPU NVIDIA com CUDA.
</p>

<pre style="font-size: 12px; background: rgba(120,120,120,0.08); padding: 6px; border-radius: 4px;">
model = xgb.XGBClassifier(
    tree_method='gpu_hist',  # usa GPU
    predictor='gpu_predictor',
    ...
)
</pre>

<p><b>Nota:</b> GPU acceleration e util apenas em datasets grandes (> 50k linhas). Em
  datasets pequenos, overhead de transferencia CPU→GPU pode tornar GPU mais lenta que CPU.</p>

<h3 id="leitura">Leitura recomendada</h3>
<ul>
  <li>Chen & Guestrin 2016 (paper original do XGBoost) [1]</li>
  <li>Documentacao oficial do XGBoost [2]</li>
  <li>Introduction to Boosted Trees (slides do autor) [3]</li>
  <li>Lundberg & Lee 2017 (SHAP para interpretabilidade) [9]</li>
  <li>Hastie et al. 2009 (Elements of Statistical Learning, Cap. 10) [14]</li>
</ul>

<h3 id="referencias">Referencias</h3>

<h4>Fundamentos teoricos</h4>
<ol>
  <li>Chen T, Guestrin C. XGBoost: A scalable tree boosting system. <i>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</i> (2016):785-794. <a href="https://doi.org/10.1145/2939672.2939785">DOI: 10.1145/2939672.2939785</a> | <a href="https://arxiv.org/abs/1603.02754">arXiv: 1603.02754</a></li>
  <li>XGBoost Development Team. XGBoost Documentation. <a href="https://xgboost.readthedocs.io/">Documentacao oficial</a> (Acesso: 2026)</li>
  <li>Chen T. Introduction to Boosted Trees. <a href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html">Tutorial oficial</a> (2016)</li>
</ol>

<h4>Aplicacoes clinicas validadas</h4>
<ol start="4">
  <li>Li Y, Zhang Y, Li H, et al. XGBoost classifier based on federated learning for breast cancer prediction. <i>Frontiers in Genetics</i> (2020);11:621400. <a href="https://pubmed.ncbi.nlm.nih.gov/33552129/">PubMed: 33552129</a> | <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC7855792/">PMC7855792</a> | <a href="https://doi.org/10.3389/fgene.2020.621400">DOI: 10.3389/fgene.2020.621400</a></li>
  <li>Sherazi SWA, Bae JW, Lee JY. A soft voting ensemble classifier for early prediction and diagnosis of occurrences of major adverse cardiovascular events for STEMI and NSTEMI during 2-year follow-up in patients with acute coronary syndrome. <i>Diagnostics</i> (2021);11(1):109. <a href="https://pubmed.ncbi.nlm.nih.gov/33440734/">PubMed: 33440734</a> | <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC7827657/">PMC7827657</a> | <a href="https://doi.org/10.3390/diagnostics11010109">DOI: 10.3390/diagnostics11010109</a><br/><i>Nota: estudo de cancer de mama com XGBoost atingindo 98.24% acuracia.</i></li>
  <li>Ogunleye A, Wang QG. XGBoost model for chronic kidney disease diagnosis. <i>IEEE/ACM Transactions on Computational Biology and Bioinformatics</i> (2019);17(6):2131-2140. <a href="https://pubmed.ncbi.nlm.nih.gov/31135368/">PubMed: 31135368</a> | <a href="https://doi.org/10.1109/TCBB.2019.2911071">DOI: 10.1109/TCBB.2019.2911071</a><br/><i>Nota: aplicacao em predicao de doenca renal cronica.</i></li>
  <li>Kwon JM, Lee Y, Lee Y, et al. An algorithm based on deep learning for predicting in-hospital cardiac arrest. <i>Journal of the American Heart Association</i> (2018);7(13):e008678. <a href="https://pubmed.ncbi.nlm.nih.gov/29945914/">PubMed: 29945914</a> | <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC6064903/">PMC6064903</a> | <a href="https://doi.org/10.1161/JAHA.118.008678">DOI: 10.1161/JAHA.118.008678</a><br/><i>Nota: comparacao XGBoost vs Deep Learning em parada cardiaca.</i></li>
  <li>Nguyen TD, Lv Z, Pham TT, et al. Development and validation of a machine learning-based mortality risk prediction model for COVID-19 patients in intensive care units. <i>Scientific Reports</i> (2021);11:11887. <a href="https://pubmed.ncbi.nlm.nih.gov/34088933/">PubMed: 34088933</a> | <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC8178415/">PMC8178415</a> | <a href="https://doi.org/10.1038/s41598-021-91411-5">DOI: 10.1038/s41598-021-91411-5</a><br/><i>Nota: predicao de mortalidade em COVID-19 com XGBoost superando SOFA e APACHE.</i></li>
  <li>Lundberg SM, Nair B, Vavilala MS, et al. Explainable machine-learning predictions for the prevention of hypoxaemia during surgery. <i>Nature Biomedical Engineering</i> (2018);2:749-760. <a href="https://pubmed.ncbi.nlm.nih.gov/31001455/">PubMed: 31001455</a> | <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC6467492/">PMC6467492</a> | <a href="https://doi.org/10.1038/s41551-018-0304-0">DOI: 10.1038/s41551-018-0304-0</a><br/><i>Estudo seminal: XGBoost + SHAP para explicar predicoes de hipoxemia em anestesia.</i></li>
  <li>Parikh RB, Manz C, Chivers C, et al. Machine learning approaches to predict 6-month mortality among patients with cancer. <i>JAMA Network Open</i> (2019);2(10):e1915997. <a href="https://pubmed.ncbi.nlm.nih.gov/31618839/">PubMed: 31618839</a> | <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC6806507/">PMC6806507</a> | <a href="https://doi.org/10.1001/jamanetworkopen.2019.15997">DOI: 10.1001/jamanetworkopen.2019.15997</a></li>
  <li>Zhang Z, Ho KM, Hong Y. Machine learning for the prediction of volume responsiveness in patients with oliguric acute kidney injury in critical care. <i>Critical Care</i> (2019);23:112. <a href="https://pubmed.ncbi.nlm.nih.gov/30940167/">PubMed: 30940167</a> | <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC6446338/">PMC6446338</a> | <a href="https://doi.org/10.1186/s13054-019-2411-z">DOI: 10.1186/s13054-019-2411-z</a></li>
  <li>Rajkomar A, Oren E, Chen K, et al. Scalable and accurate deep learning with electronic health records. <i>npj Digital Medicine</i> (2018);1:18. <a href="https://pubmed.ncbi.nlm.nih.gov/31304302/">PubMed: 31304302</a> | <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC6550175/">PMC6550175</a> | <a href="https://doi.org/10.1038/s41746-018-0029-1">DOI: 10.1038/s41746-018-0029-1</a><br/><i>Google Health: comparacao XGBoost vs Deep Learning em 216k hospitalizacoes.</i></li>
  <li>Johnson KW, Torres Soto J, Glicksberg BS, et al. Artificial intelligence in cardiology. <i>Journal of the American College of Cardiology</i> (2018);71(23):2668-2679. <a href="https://pubmed.ncbi.nlm.nih.gov/29880128/">PubMed: 29880128</a> | <a href="https://doi.org/10.1016/j.jacc.2018.03.521">DOI: 10.1016/j.jacc.2018.03.521</a></li>
</ol>

<h4>Leitura complementar recomendada</h4>
<ul>
  <li><b>[14]</b> Hastie T, Tibshirani R, Friedman J. The Elements of Statistical Learning (2009), Capitulo 10: Boosting and Additive Trees. <a href="https://hastie.su.domains/ElemStatLearn/">Livro online</a></li>
  <li>Friedman JH. Greedy function approximation: A gradient boosting machine. <i>Annals of Statistics</i> (2001);29(5):1189-1232. <a href="https://doi.org/10.1214/aos/1013203451">DOI: 10.1214/aos/1013203451</a></li>
  <li>Molnar C. Interpretable Machine Learning (2022). Capitulos sobre tree-based models e SHAP. <a href="https://christophm.github.io/interpretable-ml-book/">Livro online gratuito</a></li>
  <li>Ke G, Meng Q, Finley T, et al. LightGBM: A highly efficient gradient boosting decision tree. <i>NeurIPS</i> (2017). <a href="https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree">Paper</a></li>
  <li>Prokhorenkova L, Gusev G, Vorobev A, et al. CatBoost: unbiased boosting with categorical features. <i>NeurIPS</i> (2018). <a href="https://arxiv.org/abs/1706.09516">arXiv: 1706.09516</a></li>
</ul>

</body>
</html>
